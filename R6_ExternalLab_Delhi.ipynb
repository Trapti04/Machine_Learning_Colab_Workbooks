{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "R6_ExternalLab_Delhi.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YYk8NG3yOIT9"
      },
      "source": [
        "### A MNIST-like fashion product database\n",
        "\n",
        "In this, we classify the images into respective classes given in the dataset. We use a Neural Net and a Deep Neural Net in Keras to solve this and check the accuracy scores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tFO6PuxzOIT_"
      },
      "source": [
        "### Load tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "efNjNImfOIUC",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.set_random_seed(42)\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l9C4aAIGOIUH",
        "outputId": "eac925df-76e0-4cf4-f68b-4877319b8da7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.15.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HcoZBStrOIUQ"
      },
      "source": [
        "### Collect Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XA1WsFSeOIUS",
        "colab": {}
      },
      "source": [
        "import keras"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qnbx7TyQOIUY",
        "colab": {}
      },
      "source": [
        "(trainX, trainY), (testX, testY) = keras.datasets.fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UbiHj5YPOIUc",
        "outputId": "bd31a500-2342-407f-9d9a-8e02084e69e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(testY[0:5])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[9 2 1 1 6]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ArpgnVJSOn77"
      },
      "source": [
        "### Convert both training and testing labels into one-hot vectors.\n",
        "\n",
        "**Hint:** check **tf.keras.utils.to_categorical()**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vBlfYlANOIUk",
        "colab": {}
      },
      "source": [
        "# done after visualization"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RHV3b9mzOIUq",
        "outputId": "d53c311a-e5ed-46bb-a89f-c4e7275bfc3e",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(trainY.shape)\n",
        "print('First 5 examples now are: ', trainY[0:5])"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000,)\n",
            "First 5 examples now are:  [9 0 0 3 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FwhQ8e7VOIUw"
      },
      "source": [
        "### Visualize the data\n",
        "\n",
        "Plot first 10 images in the triaining set and their labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iBh_htTsLrRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist_labels = [\"T-shirt/top\",  # index 0\n",
        "                        \"Trouser\",      # index 1\n",
        "                        \"Pullover\",     # index 2 \n",
        "                        \"Dress\",        # index 3 \n",
        "                        \"Coat\",         # index 4\n",
        "                        \"Sandal\",       # index 5\n",
        "                        \"Shirt\",        # index 6 \n",
        "                        \"Sneaker\",      # index 7 \n",
        "                        \"Bag\",          # index 8 \n",
        "                        \"Ankle boot\"]   # index 9"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AvDML2OoOIUx",
        "outputId": "e34004c6-1c9c-47ec-996b-4a261ffd774b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for i in range(9):\n",
        "    label_index = trainY[i]\n",
        "    plt.subplot(330 + 1 + i)\n",
        "    print (\"y = \" + str(label_index) + \" \"  + (fashion_mnist_labels[label_index]))\n",
        "    plt.imshow(trainX[i],cmap='gray')\n"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y = 9 Ankle boot\n",
            "y = 0 T-shirt/top\n",
            "y = 0 T-shirt/top\n",
            "y = 3 Dress\n",
            "y = 0 T-shirt/top\n",
            "y = 2 Pullover\n",
            "y = 7 Sneaker\n",
            "y = 2 Pullover\n",
            "y = 5 Sandal\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAAD7CAYAAAAFI30bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkn+P9T4WNQ0TBthAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 9 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "l4TbJGeSOIU4"
      },
      "source": [
        "### Build a neural Network with a cross entropy loss function and sgd optimizer in Keras. The output layer with 10 neurons as we have 10 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_UKJsWYSYOT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainY = tf.keras.utils.to_categorical(trainY, num_classes=10)\n",
        "testY = tf.keras.utils.to_categorical(testY, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ac06XZZTOIU6",
        "colab": {}
      },
      "source": [
        "#Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Normalize the data\n",
        "#model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "#Comile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3hQpLv3aOIU_"
      },
      "source": [
        "### Execute the model using model.fit()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O59C_-IgOIVB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8e31ff49-e8c9-4dff-8f99-cf613d3a33ad"
      },
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=trainX.shape[0])"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 0s 6us/sample - loss: 221.8861 - acc: 0.1011 - val_loss: 6592.2783 - val_acc: 0.2763\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 6597.0093 - acc: 0.2788 - val_loss: 10709.8848 - val_acc: 0.2575\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 10728.7021 - acc: 0.2583 - val_loss: 14982.7773 - val_acc: 0.1835\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 14968.3633 - acc: 0.1848 - val_loss: 17310.0703 - val_acc: 0.3027\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 17231.1152 - acc: 0.3051 - val_loss: 17529.5078 - val_acc: 0.2381\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 17441.3301 - acc: 0.2378 - val_loss: 16466.3789 - val_acc: 0.2995\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 16391.6973 - acc: 0.2997 - val_loss: 13435.8594 - val_acc: 0.2711\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 13300.9170 - acc: 0.2711 - val_loss: 7955.0928 - val_acc: 0.2978\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 7914.0850 - acc: 0.3028 - val_loss: 9530.3623 - val_acc: 0.3924\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 9481.9033 - acc: 0.3960 - val_loss: 7416.6895 - val_acc: 0.3734\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 7349.8745 - acc: 0.3821 - val_loss: 5020.2271 - val_acc: 0.4777\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 4969.0879 - acc: 0.4786 - val_loss: 4396.4438 - val_acc: 0.4844\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 4305.1948 - acc: 0.4963 - val_loss: 5105.7876 - val_acc: 0.5041\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 5009.2881 - acc: 0.5100 - val_loss: 6594.5898 - val_acc: 0.3611\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 6508.3457 - acc: 0.3610 - val_loss: 7207.3008 - val_acc: 0.4998\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 7153.5889 - acc: 0.5015 - val_loss: 5613.1558 - val_acc: 0.5159\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 5552.6831 - acc: 0.5167 - val_loss: 5915.5361 - val_acc: 0.5184\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 5859.0732 - acc: 0.5204 - val_loss: 3656.8232 - val_acc: 0.6392\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3584.2612 - acc: 0.6449 - val_loss: 1193.1224 - val_acc: 0.6580\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1164.4377 - acc: 0.6734 - val_loss: 2002.4988 - val_acc: 0.6640\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1971.6733 - acc: 0.6781 - val_loss: 1047.0964 - val_acc: 0.6556\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1020.3072 - acc: 0.6643 - val_loss: 2469.3096 - val_acc: 0.5236\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2381.3198 - acc: 0.5313 - val_loss: 3701.3660 - val_acc: 0.6207\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3652.4343 - acc: 0.6272 - val_loss: 3583.8557 - val_acc: 0.5564\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3529.0671 - acc: 0.5604 - val_loss: 4103.9585 - val_acc: 0.6113\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 4020.3679 - acc: 0.6203 - val_loss: 2385.7896 - val_acc: 0.6270\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2341.7698 - acc: 0.6366 - val_loss: 3142.5793 - val_acc: 0.6350\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3094.1514 - acc: 0.6398 - val_loss: 3767.6440 - val_acc: 0.6357\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3696.1819 - acc: 0.6395 - val_loss: 2108.1204 - val_acc: 0.6459\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2053.2661 - acc: 0.6579 - val_loss: 3022.8589 - val_acc: 0.6675\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2973.7498 - acc: 0.6737 - val_loss: 3568.5415 - val_acc: 0.6505\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3496.2651 - acc: 0.6581 - val_loss: 1660.9111 - val_acc: 0.6643\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1624.9421 - acc: 0.6701 - val_loss: 1133.3978 - val_acc: 0.7295\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1104.1808 - acc: 0.7381 - val_loss: 1316.0708 - val_acc: 0.6266\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1241.5245 - acc: 0.6370 - val_loss: 3155.7180 - val_acc: 0.6633\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3112.1267 - acc: 0.6699 - val_loss: 3184.9751 - val_acc: 0.6020\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3114.7227 - acc: 0.6045 - val_loss: 2692.7974 - val_acc: 0.6315\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2658.2695 - acc: 0.6355 - val_loss: 3770.4319 - val_acc: 0.5661\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3647.5125 - acc: 0.5771 - val_loss: 3931.1360 - val_acc: 0.6759\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3860.8982 - acc: 0.6779 - val_loss: 3956.8159 - val_acc: 0.6375\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3897.3201 - acc: 0.6464 - val_loss: 2858.0083 - val_acc: 0.6829\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2793.5513 - acc: 0.6876 - val_loss: 1443.9001 - val_acc: 0.6469\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1384.1125 - acc: 0.6583 - val_loss: 2221.7876 - val_acc: 0.7159\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2184.9312 - acc: 0.7274 - val_loss: 1367.5052 - val_acc: 0.6946\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1319.8616 - acc: 0.6959 - val_loss: 1930.3264 - val_acc: 0.6489\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1861.9989 - acc: 0.6572 - val_loss: 1799.0396 - val_acc: 0.6535\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1741.1992 - acc: 0.6568 - val_loss: 2013.1976 - val_acc: 0.6495\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1936.2698 - acc: 0.6575 - val_loss: 1763.7200 - val_acc: 0.6860\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1710.0806 - acc: 0.6887 - val_loss: 2106.6616 - val_acc: 0.6601\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2058.7390 - acc: 0.6653 - val_loss: 2584.2236 - val_acc: 0.6519\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2474.8186 - acc: 0.6647 - val_loss: 2816.5603 - val_acc: 0.6955\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2757.3320 - acc: 0.7005 - val_loss: 2814.4177 - val_acc: 0.6662\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2763.8447 - acc: 0.6730 - val_loss: 1909.0206 - val_acc: 0.7223\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1848.9885 - acc: 0.7313 - val_loss: 743.6001 - val_acc: 0.7337\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 705.9721 - acc: 0.7466 - val_loss: 669.3771 - val_acc: 0.7537\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 632.8912 - acc: 0.7670 - val_loss: 693.2169 - val_acc: 0.7508\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 650.8832 - acc: 0.7624 - val_loss: 831.6204 - val_acc: 0.7321\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 783.9176 - acc: 0.7417 - val_loss: 1543.2900 - val_acc: 0.6792\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1481.4280 - acc: 0.6843 - val_loss: 1482.2202 - val_acc: 0.7003\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1427.0072 - acc: 0.7054 - val_loss: 1969.1244 - val_acc: 0.6449\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1893.5474 - acc: 0.6577 - val_loss: 2400.5281 - val_acc: 0.7093\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2356.0703 - acc: 0.7199 - val_loss: 1759.6312 - val_acc: 0.6607\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1701.1085 - acc: 0.6669 - val_loss: 2262.1189 - val_acc: 0.6301\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2215.7078 - acc: 0.6373 - val_loss: 2601.5432 - val_acc: 0.6443\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2485.4543 - acc: 0.6618 - val_loss: 3049.9929 - val_acc: 0.6971\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2991.2585 - acc: 0.7029 - val_loss: 3074.3364 - val_acc: 0.6749\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3019.5754 - acc: 0.6815 - val_loss: 1869.2334 - val_acc: 0.7279\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1810.1217 - acc: 0.7330 - val_loss: 1501.9331 - val_acc: 0.6539\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1418.6439 - acc: 0.6690 - val_loss: 2798.4539 - val_acc: 0.7274\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2759.6482 - acc: 0.7346 - val_loss: 2121.9038 - val_acc: 0.6973\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2066.1907 - acc: 0.7020 - val_loss: 1968.8132 - val_acc: 0.6769\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1923.5228 - acc: 0.6828 - val_loss: 3182.0520 - val_acc: 0.5873\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3049.5952 - acc: 0.5989 - val_loss: 3738.8821 - val_acc: 0.6678\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3671.2996 - acc: 0.6731 - val_loss: 3463.8372 - val_acc: 0.6558\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 3405.8918 - acc: 0.6593 - val_loss: 2121.3467 - val_acc: 0.6799\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2057.3269 - acc: 0.6846 - val_loss: 1420.9824 - val_acc: 0.6846\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1362.5518 - acc: 0.6949 - val_loss: 1488.1262 - val_acc: 0.7433\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1448.9696 - acc: 0.7526 - val_loss: 617.1098 - val_acc: 0.7650\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 581.6364 - acc: 0.7765 - val_loss: 654.1687 - val_acc: 0.7720\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 609.7195 - acc: 0.7825 - val_loss: 853.0342 - val_acc: 0.7163\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 793.1002 - acc: 0.7293 - val_loss: 1702.7852 - val_acc: 0.7012\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1654.3510 - acc: 0.7084 - val_loss: 1413.6270 - val_acc: 0.6971\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1349.9352 - acc: 0.7056 - val_loss: 1285.2319 - val_acc: 0.6798\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1234.1462 - acc: 0.6881 - val_loss: 2115.1245 - val_acc: 0.6321\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2010.0765 - acc: 0.6434 - val_loss: 2524.3237 - val_acc: 0.7207\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2480.8018 - acc: 0.7293 - val_loss: 1409.7770 - val_acc: 0.7349\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1365.7130 - acc: 0.7466 - val_loss: 634.3540 - val_acc: 0.7373\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 595.7990 - acc: 0.7529 - val_loss: 620.3056 - val_acc: 0.7750\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 579.3974 - acc: 0.7874 - val_loss: 629.7853 - val_acc: 0.7516\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 576.1748 - acc: 0.7673 - val_loss: 1215.9554 - val_acc: 0.7632\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1186.0122 - acc: 0.7703 - val_loss: 713.0868 - val_acc: 0.7388\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 651.2864 - acc: 0.7541 - val_loss: 1665.9493 - val_acc: 0.7461\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1634.8579 - acc: 0.7552 - val_loss: 1095.1504 - val_acc: 0.7258\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1043.8790 - acc: 0.7333 - val_loss: 1794.8586 - val_acc: 0.6971\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1738.9626 - acc: 0.7060 - val_loss: 1016.5348 - val_acc: 0.7270\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 972.9656 - acc: 0.7346 - val_loss: 1165.8445 - val_acc: 0.6814\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1090.4318 - acc: 0.6934 - val_loss: 1991.7852 - val_acc: 0.6884\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1945.8134 - acc: 0.6968 - val_loss: 2050.1079 - val_acc: 0.6550\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 1996.4971 - acc: 0.6582 - val_loss: 2502.6196 - val_acc: 0.6279\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 0s 4us/sample - loss: 2389.8123 - acc: 0.6420 - val_loss: 3024.0042 - val_acc: 0.6781\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffaf19fa160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JdzDtGwDOIVF"
      },
      "source": [
        "### In the above Neural Network model add Batch Normalization layer after the input layer and repeat the steps."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kndfpdidOIVI",
        "colab": {}
      },
      "source": [
        "#Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "#Comile the model\n",
        "model.compile(optimizer='sgd', loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mwk3T5LJOIVN"
      },
      "source": [
        "### Execute the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JNLR8tcBOIVP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a90a7004-425a-47d0-8a62-d9b6af5147de"
      },
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=trainX.shape[0])"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 1s 15us/sample - loss: 3.0905 - acc: 0.1022 - val_loss: 20.5801 - val_acc: 0.1639\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.7832 - acc: 0.1422 - val_loss: 13.3474 - val_acc: 0.2038\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.5349 - acc: 0.1806 - val_loss: 10.2218 - val_acc: 0.2179\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.3320 - acc: 0.2217 - val_loss: 8.3411 - val_acc: 0.2275\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.1641 - acc: 0.2623 - val_loss: 7.0430 - val_acc: 0.2407\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.0238 - acc: 0.3040 - val_loss: 6.0815 - val_acc: 0.2549\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.9057 - acc: 0.3438 - val_loss: 5.3406 - val_acc: 0.2716\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.8056 - acc: 0.3778 - val_loss: 4.7564 - val_acc: 0.2895\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7204 - acc: 0.4083 - val_loss: 4.2881 - val_acc: 0.3107\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6472 - acc: 0.4344 - val_loss: 3.9075 - val_acc: 0.3310\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.5841 - acc: 0.4559 - val_loss: 3.5942 - val_acc: 0.3551\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.5290 - acc: 0.4746 - val_loss: 3.3333 - val_acc: 0.3757\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.4807 - acc: 0.4918 - val_loss: 3.1133 - val_acc: 0.3943\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.4380 - acc: 0.5065 - val_loss: 2.9259 - val_acc: 0.4117\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.3998 - acc: 0.5189 - val_loss: 2.7646 - val_acc: 0.4285\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.3655 - acc: 0.5305 - val_loss: 2.6242 - val_acc: 0.4459\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.3344 - acc: 0.5411 - val_loss: 2.5011 - val_acc: 0.4606\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.3061 - acc: 0.5506 - val_loss: 2.3920 - val_acc: 0.4729\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.2802 - acc: 0.5579 - val_loss: 2.2947 - val_acc: 0.4866\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.2564 - acc: 0.5658 - val_loss: 2.2073 - val_acc: 0.4969\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.2343 - acc: 0.5734 - val_loss: 2.1283 - val_acc: 0.5073\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.2138 - acc: 0.5811 - val_loss: 2.0565 - val_acc: 0.5155\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.1948 - acc: 0.5876 - val_loss: 1.9909 - val_acc: 0.5258\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.1769 - acc: 0.5935 - val_loss: 1.9307 - val_acc: 0.5350\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.1602 - acc: 0.5994 - val_loss: 1.8752 - val_acc: 0.5426\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.1445 - acc: 0.6048 - val_loss: 1.8239 - val_acc: 0.5496\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.1297 - acc: 0.6097 - val_loss: 1.7764 - val_acc: 0.5566\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.1157 - acc: 0.6141 - val_loss: 1.7321 - val_acc: 0.5640\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.1024 - acc: 0.6192 - val_loss: 1.6908 - val_acc: 0.5695\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.0898 - acc: 0.6235 - val_loss: 1.6522 - val_acc: 0.5734\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.0779 - acc: 0.6275 - val_loss: 1.6159 - val_acc: 0.5776\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.0665 - acc: 0.6313 - val_loss: 1.5819 - val_acc: 0.5824\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.0557 - acc: 0.6351 - val_loss: 1.5498 - val_acc: 0.5858\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.0453 - acc: 0.6384 - val_loss: 1.5195 - val_acc: 0.5906\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.0355 - acc: 0.6416 - val_loss: 1.4910 - val_acc: 0.5954\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.0260 - acc: 0.6446 - val_loss: 1.4639 - val_acc: 0.5994\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.0169 - acc: 0.6478 - val_loss: 1.4382 - val_acc: 0.6027\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.0082 - acc: 0.6506 - val_loss: 1.4138 - val_acc: 0.6062\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9999 - acc: 0.6534 - val_loss: 1.3906 - val_acc: 0.6104\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9919 - acc: 0.6560 - val_loss: 1.3686 - val_acc: 0.6132\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9841 - acc: 0.6585 - val_loss: 1.3475 - val_acc: 0.6162\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9767 - acc: 0.6610 - val_loss: 1.3274 - val_acc: 0.6182\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9695 - acc: 0.6636 - val_loss: 1.3082 - val_acc: 0.6213\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9626 - acc: 0.6659 - val_loss: 1.2898 - val_acc: 0.6238\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9559 - acc: 0.6681 - val_loss: 1.2722 - val_acc: 0.6262\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9495 - acc: 0.6702 - val_loss: 1.2554 - val_acc: 0.6290\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9432 - acc: 0.6725 - val_loss: 1.2392 - val_acc: 0.6309\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9372 - acc: 0.6742 - val_loss: 1.2237 - val_acc: 0.6338\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9313 - acc: 0.6760 - val_loss: 1.2087 - val_acc: 0.6363\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9257 - acc: 0.6781 - val_loss: 1.1944 - val_acc: 0.6388\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9202 - acc: 0.6801 - val_loss: 1.1805 - val_acc: 0.6418\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9148 - acc: 0.6817 - val_loss: 1.1672 - val_acc: 0.6435\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9096 - acc: 0.6834 - val_loss: 1.1544 - val_acc: 0.6454\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.9046 - acc: 0.6854 - val_loss: 1.1420 - val_acc: 0.6464\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8997 - acc: 0.6873 - val_loss: 1.1301 - val_acc: 0.6482\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8950 - acc: 0.6889 - val_loss: 1.1186 - val_acc: 0.6513\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8903 - acc: 0.6910 - val_loss: 1.1074 - val_acc: 0.6534\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8858 - acc: 0.6923 - val_loss: 1.0966 - val_acc: 0.6547\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8814 - acc: 0.6940 - val_loss: 1.0862 - val_acc: 0.6562\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8772 - acc: 0.6956 - val_loss: 1.0762 - val_acc: 0.6588\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8730 - acc: 0.6970 - val_loss: 1.0664 - val_acc: 0.6612\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8689 - acc: 0.6985 - val_loss: 1.0569 - val_acc: 0.6633\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8650 - acc: 0.6999 - val_loss: 1.0478 - val_acc: 0.6647\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8611 - acc: 0.7010 - val_loss: 1.0389 - val_acc: 0.6680\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8573 - acc: 0.7023 - val_loss: 1.0303 - val_acc: 0.6692\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8536 - acc: 0.7035 - val_loss: 1.0220 - val_acc: 0.6712\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8500 - acc: 0.7049 - val_loss: 1.0139 - val_acc: 0.6731\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8465 - acc: 0.7061 - val_loss: 1.0060 - val_acc: 0.6749\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8430 - acc: 0.7074 - val_loss: 0.9983 - val_acc: 0.6768\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8397 - acc: 0.7089 - val_loss: 0.9909 - val_acc: 0.6790\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 0.8364 - acc: 0.7102 - val_loss: 0.9837 - val_acc: 0.6804\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8331 - acc: 0.7113 - val_loss: 0.9767 - val_acc: 0.6824\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8300 - acc: 0.7124 - val_loss: 0.9699 - val_acc: 0.6836\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8269 - acc: 0.7135 - val_loss: 0.9632 - val_acc: 0.6845\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8238 - acc: 0.7145 - val_loss: 0.9568 - val_acc: 0.6866\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8209 - acc: 0.7154 - val_loss: 0.9505 - val_acc: 0.6883\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8179 - acc: 0.7163 - val_loss: 0.9444 - val_acc: 0.6898\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8151 - acc: 0.7173 - val_loss: 0.9384 - val_acc: 0.6921\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8123 - acc: 0.7183 - val_loss: 0.9326 - val_acc: 0.6932\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8095 - acc: 0.7193 - val_loss: 0.9269 - val_acc: 0.6944\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8068 - acc: 0.7202 - val_loss: 0.9214 - val_acc: 0.6959\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8042 - acc: 0.7212 - val_loss: 0.9161 - val_acc: 0.6965\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.8016 - acc: 0.7221 - val_loss: 0.9108 - val_acc: 0.6972\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7990 - acc: 0.7232 - val_loss: 0.9057 - val_acc: 0.6990\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7965 - acc: 0.7241 - val_loss: 0.9007 - val_acc: 0.7003\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7941 - acc: 0.7248 - val_loss: 0.8958 - val_acc: 0.7013\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7916 - acc: 0.7256 - val_loss: 0.8911 - val_acc: 0.7029\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7893 - acc: 0.7265 - val_loss: 0.8865 - val_acc: 0.7040\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7869 - acc: 0.7273 - val_loss: 0.8819 - val_acc: 0.7054\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7846 - acc: 0.7282 - val_loss: 0.8775 - val_acc: 0.7063\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7824 - acc: 0.7289 - val_loss: 0.8732 - val_acc: 0.7066\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7801 - acc: 0.7298 - val_loss: 0.8690 - val_acc: 0.7086\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7779 - acc: 0.7307 - val_loss: 0.8649 - val_acc: 0.7102\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7758 - acc: 0.7315 - val_loss: 0.8608 - val_acc: 0.7114\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7737 - acc: 0.7324 - val_loss: 0.8569 - val_acc: 0.7126\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7716 - acc: 0.7332 - val_loss: 0.8530 - val_acc: 0.7135\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7695 - acc: 0.7341 - val_loss: 0.8493 - val_acc: 0.7149\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7675 - acc: 0.7348 - val_loss: 0.8456 - val_acc: 0.7160\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7655 - acc: 0.7354 - val_loss: 0.8420 - val_acc: 0.7168\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 0.7636 - acc: 0.7364 - val_loss: 0.8385 - val_acc: 0.7170\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffaf1c10da0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Py-KwkmjOIVU"
      },
      "source": [
        "### Customize the learning rate to 0.001 in sgd optimizer and run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yLXUE9jWOIVV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aa15b5f8-7e8e-42be-ac53-10faf5770e6a"
      },
      "source": [
        "#Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "#Add Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "# create a new optimizer\n",
        "opt_ = tf.keras.optimizers.SGD(lr = 0.001)\n",
        "print(opt_)\n",
        "#Compile the model\n",
        "model.compile(optimizer = opt_ , loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7ffaeb1b0f98>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pJUqA5T4OIVc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "049a3075-b69c-4e27-f226-bb7a2e45764f"
      },
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=100,\n",
        "          batch_size=trainX.shape[0])"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 1s 19us/sample - loss: 2.9343 - acc: 0.1328 - val_loss: 19.4616 - val_acc: 0.1115\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.9092 - acc: 0.1357 - val_loss: 13.8171 - val_acc: 0.1138\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.8844 - acc: 0.1386 - val_loss: 11.2709 - val_acc: 0.1168\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.8600 - acc: 0.1411 - val_loss: 9.7380 - val_acc: 0.1200\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.8360 - acc: 0.1439 - val_loss: 8.6847 - val_acc: 0.1230\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.8123 - acc: 0.1469 - val_loss: 7.9029 - val_acc: 0.1258\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.7889 - acc: 0.1499 - val_loss: 7.2927 - val_acc: 0.1279\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.7659 - acc: 0.1529 - val_loss: 6.7991 - val_acc: 0.1306\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.7433 - acc: 0.1553 - val_loss: 6.3889 - val_acc: 0.1335\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.7209 - acc: 0.1586 - val_loss: 6.0410 - val_acc: 0.1367\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.6989 - acc: 0.1617 - val_loss: 5.7411 - val_acc: 0.1392\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.6773 - acc: 0.1653 - val_loss: 5.4789 - val_acc: 0.1423\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.6559 - acc: 0.1684 - val_loss: 5.2472 - val_acc: 0.1451\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.6349 - acc: 0.1717 - val_loss: 5.0405 - val_acc: 0.1493\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.6142 - acc: 0.1753 - val_loss: 4.8545 - val_acc: 0.1519\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.5938 - acc: 0.1793 - val_loss: 4.6861 - val_acc: 0.1554\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.5737 - acc: 0.1828 - val_loss: 4.5326 - val_acc: 0.1585\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.5539 - acc: 0.1863 - val_loss: 4.3920 - val_acc: 0.1613\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.5344 - acc: 0.1899 - val_loss: 4.2625 - val_acc: 0.1651\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.5152 - acc: 0.1935 - val_loss: 4.1428 - val_acc: 0.1693\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.4963 - acc: 0.1973 - val_loss: 4.0317 - val_acc: 0.1736\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.4777 - acc: 0.2014 - val_loss: 3.9282 - val_acc: 0.1761\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.4593 - acc: 0.2050 - val_loss: 3.8315 - val_acc: 0.1797\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.4413 - acc: 0.2092 - val_loss: 3.7409 - val_acc: 0.1828\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.4235 - acc: 0.2132 - val_loss: 3.6558 - val_acc: 0.1868\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 2.4060 - acc: 0.2169 - val_loss: 3.5756 - val_acc: 0.1903\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.3887 - acc: 0.2209 - val_loss: 3.4999 - val_acc: 0.1934\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.3717 - acc: 0.2250 - val_loss: 3.4283 - val_acc: 0.1970\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.3550 - acc: 0.2288 - val_loss: 3.3604 - val_acc: 0.2005\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.3385 - acc: 0.2328 - val_loss: 3.2960 - val_acc: 0.2044\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.3223 - acc: 0.2371 - val_loss: 3.2347 - val_acc: 0.2080\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.3063 - acc: 0.2413 - val_loss: 3.1763 - val_acc: 0.2112\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2905 - acc: 0.2457 - val_loss: 3.1206 - val_acc: 0.2146\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2750 - acc: 0.2500 - val_loss: 3.0674 - val_acc: 0.2176\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2598 - acc: 0.2544 - val_loss: 3.0165 - val_acc: 0.2210\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2447 - acc: 0.2585 - val_loss: 2.9678 - val_acc: 0.2242\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2299 - acc: 0.2631 - val_loss: 2.9211 - val_acc: 0.2275\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2153 - acc: 0.2672 - val_loss: 2.8763 - val_acc: 0.2295\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.2010 - acc: 0.2717 - val_loss: 2.8333 - val_acc: 0.2325\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.1868 - acc: 0.2761 - val_loss: 2.7919 - val_acc: 0.2348\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.1729 - acc: 0.2801 - val_loss: 2.7520 - val_acc: 0.2397\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.1592 - acc: 0.2847 - val_loss: 2.7137 - val_acc: 0.2432\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.1457 - acc: 0.2887 - val_loss: 2.6767 - val_acc: 0.2471\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.1323 - acc: 0.2925 - val_loss: 2.6410 - val_acc: 0.2506\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.1192 - acc: 0.2962 - val_loss: 2.6065 - val_acc: 0.2545\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.1063 - acc: 0.3005 - val_loss: 2.5733 - val_acc: 0.2586\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.0936 - acc: 0.3046 - val_loss: 2.5411 - val_acc: 0.2621\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.0810 - acc: 0.3086 - val_loss: 2.5100 - val_acc: 0.2651\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.0687 - acc: 0.3128 - val_loss: 2.4798 - val_acc: 0.2701\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.0565 - acc: 0.3169 - val_loss: 2.4507 - val_acc: 0.2737\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.0445 - acc: 0.3210 - val_loss: 2.4224 - val_acc: 0.2762\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.0327 - acc: 0.3246 - val_loss: 2.3950 - val_acc: 0.2803\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.0210 - acc: 0.3283 - val_loss: 2.3685 - val_acc: 0.2845\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 2.0096 - acc: 0.3318 - val_loss: 2.3427 - val_acc: 0.2896\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.9982 - acc: 0.3354 - val_loss: 2.3176 - val_acc: 0.2941\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.9871 - acc: 0.3394 - val_loss: 2.2933 - val_acc: 0.2982\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9761 - acc: 0.3428 - val_loss: 2.2697 - val_acc: 0.3017\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.9653 - acc: 0.3460 - val_loss: 2.2468 - val_acc: 0.3048\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.9546 - acc: 0.3492 - val_loss: 2.2245 - val_acc: 0.3083\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.9441 - acc: 0.3526 - val_loss: 2.2028 - val_acc: 0.3114\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.9337 - acc: 0.3564 - val_loss: 2.1816 - val_acc: 0.3154\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.9235 - acc: 0.3598 - val_loss: 2.1611 - val_acc: 0.3196\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.9134 - acc: 0.3630 - val_loss: 2.1411 - val_acc: 0.3239\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.9035 - acc: 0.3664 - val_loss: 2.1216 - val_acc: 0.3278\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.8937 - acc: 0.3696 - val_loss: 2.1026 - val_acc: 0.3323\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.8840 - acc: 0.3727 - val_loss: 2.0841 - val_acc: 0.3364\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.8745 - acc: 0.3757 - val_loss: 2.0661 - val_acc: 0.3406\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.8651 - acc: 0.3790 - val_loss: 2.0485 - val_acc: 0.3437\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.8559 - acc: 0.3818 - val_loss: 2.0313 - val_acc: 0.3474\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.8467 - acc: 0.3848 - val_loss: 2.0146 - val_acc: 0.3513\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.8377 - acc: 0.3876 - val_loss: 1.9983 - val_acc: 0.3547\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.8288 - acc: 0.3903 - val_loss: 1.9823 - val_acc: 0.3578\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.8201 - acc: 0.3935 - val_loss: 1.9668 - val_acc: 0.3610\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.8114 - acc: 0.3962 - val_loss: 1.9516 - val_acc: 0.3645\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.8029 - acc: 0.3987 - val_loss: 1.9368 - val_acc: 0.3683\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7945 - acc: 0.4013 - val_loss: 1.9223 - val_acc: 0.3718\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7862 - acc: 0.4037 - val_loss: 1.9082 - val_acc: 0.3767\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7780 - acc: 0.4066 - val_loss: 1.8943 - val_acc: 0.3813\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7699 - acc: 0.4091 - val_loss: 1.8808 - val_acc: 0.3851\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7620 - acc: 0.4119 - val_loss: 1.8676 - val_acc: 0.3884\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7541 - acc: 0.4141 - val_loss: 1.8547 - val_acc: 0.3912\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7463 - acc: 0.4170 - val_loss: 1.8421 - val_acc: 0.3938\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7387 - acc: 0.4195 - val_loss: 1.8297 - val_acc: 0.3974\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7311 - acc: 0.4220 - val_loss: 1.8176 - val_acc: 0.4015\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7236 - acc: 0.4247 - val_loss: 1.8058 - val_acc: 0.4048\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7163 - acc: 0.4270 - val_loss: 1.7942 - val_acc: 0.4079\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.7090 - acc: 0.4291 - val_loss: 1.7829 - val_acc: 0.4110\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.7018 - acc: 0.4319 - val_loss: 1.7718 - val_acc: 0.4140\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6948 - acc: 0.4341 - val_loss: 1.7609 - val_acc: 0.4174\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 1s 12us/sample - loss: 1.6878 - acc: 0.4365 - val_loss: 1.7503 - val_acc: 0.4195\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6809 - acc: 0.4390 - val_loss: 1.7399 - val_acc: 0.4222\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6740 - acc: 0.4412 - val_loss: 1.7297 - val_acc: 0.4244\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6673 - acc: 0.4438 - val_loss: 1.7197 - val_acc: 0.4281\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6607 - acc: 0.4460 - val_loss: 1.7099 - val_acc: 0.4310\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6541 - acc: 0.4478 - val_loss: 1.7003 - val_acc: 0.4343\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6476 - acc: 0.4498 - val_loss: 1.6909 - val_acc: 0.4372\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6412 - acc: 0.4520 - val_loss: 1.6817 - val_acc: 0.4402\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6349 - acc: 0.4542 - val_loss: 1.6727 - val_acc: 0.4431\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6287 - acc: 0.4563 - val_loss: 1.6638 - val_acc: 0.4464\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 1s 11us/sample - loss: 1.6225 - acc: 0.4584 - val_loss: 1.6551 - val_acc: 0.4494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffaeb153a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "j9CSqKvpOIVk"
      },
      "source": [
        "### Build the Neural Network model with 3 Dense layers with 100,100,10 neurons respectively in each layer. Use cross entropy loss function and singmoid as activation in the hidden layers and softmax as activation function in the output layer. Use sgd optimizer with learning rate 0.03."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GGAad54JOIVm",
        "colab": {}
      },
      "source": [
        "#Initialize Sequential model\n",
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "#Reshape data from 2D to 1D -> 28x28 to 784\n",
        "model.add(tf.keras.layers.Reshape((784,),input_shape=(28,28,)))\n",
        "\n",
        "#Normalize the data\n",
        "model.add(tf.keras.layers.BatchNormalization())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MQ7oIymROIVp",
        "colab": {}
      },
      "source": [
        "#Add 1st hidden layer\n",
        "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X-O-fFxnOIVt",
        "colab": {}
      },
      "source": [
        "#Add 2nd hidden layer\n",
        "model.add(tf.keras.layers.Dense(100, activation='sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKwIaluig9WL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Add 3rd Dense Layer which provides 10 Outputs after applying softmax\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cybcOgyRhGS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "641e98fb-0f50-4479-dbc1-44798b32d2b0"
      },
      "source": [
        "# create a new optimizer\n",
        "opt_ = tf.keras.optimizers.SGD(lr = 0.03)\n",
        "print(opt_)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tensorflow.python.keras.optimizer_v2.gradient_descent.SGD object at 0x7ffaeb0874e0>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BiP7IL52OIVw",
        "colab": {}
      },
      "source": [
        "#Compile the model\n",
        "model.compile(optimizer = opt_ , loss='categorical_crossentropy', \n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Nr2YsZV0OIV0"
      },
      "source": [
        "## Review model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h4ojW6-oOIV2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 319
        },
        "outputId": "561e01b4-44ba-453b-8a9d-25b2b136be04"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "reshape_18 (Reshape)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 784)               3136      \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 100)               78500     \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 10)                1010      \n",
            "=================================================================\n",
            "Total params: 92,746\n",
            "Trainable params: 91,178\n",
            "Non-trainable params: 1,568\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gfFGmbZLOIV5"
      },
      "source": [
        "### Run the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bIkbMEN5OIV7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "cedd73a3-02f0-4bb7-8a5a-e3bf0444c5cf"
      },
      "source": [
        "model.fit(trainX, trainY, \n",
        "          validation_data=(testX, testY), \n",
        "          epochs=200,\n",
        "          batch_size=trainX.shape[0])"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/200\n",
            "60000/60000 [==============================] - 1s 24us/sample - loss: 1.9823 - acc: 0.5889 - val_loss: 1.9646 - val_acc: 0.5544\n",
            "Epoch 2/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9793 - acc: 0.5901 - val_loss: 1.9616 - val_acc: 0.5559\n",
            "Epoch 3/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9764 - acc: 0.5911 - val_loss: 1.9585 - val_acc: 0.5572\n",
            "Epoch 4/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9734 - acc: 0.5922 - val_loss: 1.9555 - val_acc: 0.5582\n",
            "Epoch 5/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9705 - acc: 0.5931 - val_loss: 1.9524 - val_acc: 0.5598\n",
            "Epoch 6/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9675 - acc: 0.5942 - val_loss: 1.9494 - val_acc: 0.5604\n",
            "Epoch 7/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9646 - acc: 0.5953 - val_loss: 1.9464 - val_acc: 0.5623\n",
            "Epoch 8/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.9617 - acc: 0.5963 - val_loss: 1.9434 - val_acc: 0.5638\n",
            "Epoch 9/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9587 - acc: 0.5974 - val_loss: 1.9404 - val_acc: 0.5649\n",
            "Epoch 10/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9558 - acc: 0.5983 - val_loss: 1.9374 - val_acc: 0.5658\n",
            "Epoch 11/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9529 - acc: 0.5993 - val_loss: 1.9344 - val_acc: 0.5669\n",
            "Epoch 12/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.9500 - acc: 0.6004 - val_loss: 1.9314 - val_acc: 0.5683\n",
            "Epoch 13/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.9471 - acc: 0.6015 - val_loss: 1.9284 - val_acc: 0.5688\n",
            "Epoch 14/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9441 - acc: 0.6024 - val_loss: 1.9255 - val_acc: 0.5701\n",
            "Epoch 15/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.9412 - acc: 0.6032 - val_loss: 1.9225 - val_acc: 0.5712\n",
            "Epoch 16/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.9383 - acc: 0.6038 - val_loss: 1.9196 - val_acc: 0.5720\n",
            "Epoch 17/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.9355 - acc: 0.6046 - val_loss: 1.9167 - val_acc: 0.5744\n",
            "Epoch 18/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.9326 - acc: 0.6053 - val_loss: 1.9137 - val_acc: 0.5754\n",
            "Epoch 19/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9297 - acc: 0.6061 - val_loss: 1.9108 - val_acc: 0.5765\n",
            "Epoch 20/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9268 - acc: 0.6069 - val_loss: 1.9079 - val_acc: 0.5772\n",
            "Epoch 21/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9239 - acc: 0.6077 - val_loss: 1.9050 - val_acc: 0.5782\n",
            "Epoch 22/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9211 - acc: 0.6085 - val_loss: 1.9021 - val_acc: 0.5791\n",
            "Epoch 23/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9182 - acc: 0.6092 - val_loss: 1.8992 - val_acc: 0.5798\n",
            "Epoch 24/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9153 - acc: 0.6100 - val_loss: 1.8963 - val_acc: 0.5815\n",
            "Epoch 25/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9125 - acc: 0.6108 - val_loss: 1.8935 - val_acc: 0.5826\n",
            "Epoch 26/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9096 - acc: 0.6116 - val_loss: 1.8906 - val_acc: 0.5840\n",
            "Epoch 27/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9068 - acc: 0.6122 - val_loss: 1.8878 - val_acc: 0.5856\n",
            "Epoch 28/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.9039 - acc: 0.6129 - val_loss: 1.8849 - val_acc: 0.5865\n",
            "Epoch 29/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.9011 - acc: 0.6137 - val_loss: 1.8821 - val_acc: 0.5881\n",
            "Epoch 30/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8983 - acc: 0.6143 - val_loss: 1.8792 - val_acc: 0.5887\n",
            "Epoch 31/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8954 - acc: 0.6147 - val_loss: 1.8764 - val_acc: 0.5897\n",
            "Epoch 32/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8926 - acc: 0.6153 - val_loss: 1.8736 - val_acc: 0.5907\n",
            "Epoch 33/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8898 - acc: 0.6161 - val_loss: 1.8708 - val_acc: 0.5916\n",
            "Epoch 34/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8870 - acc: 0.6167 - val_loss: 1.8680 - val_acc: 0.5926\n",
            "Epoch 35/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8841 - acc: 0.6175 - val_loss: 1.8652 - val_acc: 0.5939\n",
            "Epoch 36/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8813 - acc: 0.6182 - val_loss: 1.8624 - val_acc: 0.5952\n",
            "Epoch 37/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8785 - acc: 0.6191 - val_loss: 1.8596 - val_acc: 0.5969\n",
            "Epoch 38/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8757 - acc: 0.6196 - val_loss: 1.8568 - val_acc: 0.5979\n",
            "Epoch 39/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8729 - acc: 0.6203 - val_loss: 1.8541 - val_acc: 0.5999\n",
            "Epoch 40/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8701 - acc: 0.6209 - val_loss: 1.8513 - val_acc: 0.6002\n",
            "Epoch 41/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8674 - acc: 0.6216 - val_loss: 1.8486 - val_acc: 0.6010\n",
            "Epoch 42/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8646 - acc: 0.6224 - val_loss: 1.8458 - val_acc: 0.6020\n",
            "Epoch 43/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8618 - acc: 0.6229 - val_loss: 1.8431 - val_acc: 0.6031\n",
            "Epoch 44/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8590 - acc: 0.6237 - val_loss: 1.8403 - val_acc: 0.6042\n",
            "Epoch 45/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8563 - acc: 0.6244 - val_loss: 1.8376 - val_acc: 0.6048\n",
            "Epoch 46/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8535 - acc: 0.6250 - val_loss: 1.8349 - val_acc: 0.6057\n",
            "Epoch 47/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8507 - acc: 0.6257 - val_loss: 1.8322 - val_acc: 0.6062\n",
            "Epoch 48/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8480 - acc: 0.6264 - val_loss: 1.8295 - val_acc: 0.6076\n",
            "Epoch 49/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8452 - acc: 0.6271 - val_loss: 1.8268 - val_acc: 0.6080\n",
            "Epoch 50/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8425 - acc: 0.6281 - val_loss: 1.8241 - val_acc: 0.6098\n",
            "Epoch 51/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8397 - acc: 0.6290 - val_loss: 1.8214 - val_acc: 0.6112\n",
            "Epoch 52/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8370 - acc: 0.6295 - val_loss: 1.8187 - val_acc: 0.6116\n",
            "Epoch 53/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8343 - acc: 0.6302 - val_loss: 1.8160 - val_acc: 0.6123\n",
            "Epoch 54/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8315 - acc: 0.6309 - val_loss: 1.8134 - val_acc: 0.6138\n",
            "Epoch 55/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8288 - acc: 0.6316 - val_loss: 1.8107 - val_acc: 0.6151\n",
            "Epoch 56/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8261 - acc: 0.6323 - val_loss: 1.8080 - val_acc: 0.6156\n",
            "Epoch 57/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8234 - acc: 0.6328 - val_loss: 1.8054 - val_acc: 0.6163\n",
            "Epoch 58/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8207 - acc: 0.6334 - val_loss: 1.8028 - val_acc: 0.6173\n",
            "Epoch 59/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8179 - acc: 0.6341 - val_loss: 1.8001 - val_acc: 0.6184\n",
            "Epoch 60/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8152 - acc: 0.6348 - val_loss: 1.7975 - val_acc: 0.6192\n",
            "Epoch 61/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8125 - acc: 0.6357 - val_loss: 1.7949 - val_acc: 0.6203\n",
            "Epoch 62/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8098 - acc: 0.6362 - val_loss: 1.7922 - val_acc: 0.6218\n",
            "Epoch 63/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.8072 - acc: 0.6366 - val_loss: 1.7896 - val_acc: 0.6234\n",
            "Epoch 64/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8045 - acc: 0.6374 - val_loss: 1.7870 - val_acc: 0.6242\n",
            "Epoch 65/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.8018 - acc: 0.6381 - val_loss: 1.7844 - val_acc: 0.6246\n",
            "Epoch 66/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7991 - acc: 0.6388 - val_loss: 1.7818 - val_acc: 0.6258\n",
            "Epoch 67/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7964 - acc: 0.6394 - val_loss: 1.7792 - val_acc: 0.6266\n",
            "Epoch 68/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7938 - acc: 0.6400 - val_loss: 1.7767 - val_acc: 0.6271\n",
            "Epoch 69/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7911 - acc: 0.6405 - val_loss: 1.7741 - val_acc: 0.6278\n",
            "Epoch 70/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7884 - acc: 0.6412 - val_loss: 1.7715 - val_acc: 0.6292\n",
            "Epoch 71/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7858 - acc: 0.6418 - val_loss: 1.7689 - val_acc: 0.6302\n",
            "Epoch 72/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7831 - acc: 0.6421 - val_loss: 1.7664 - val_acc: 0.6304\n",
            "Epoch 73/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7805 - acc: 0.6425 - val_loss: 1.7638 - val_acc: 0.6312\n",
            "Epoch 74/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7778 - acc: 0.6429 - val_loss: 1.7613 - val_acc: 0.6323\n",
            "Epoch 75/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7752 - acc: 0.6431 - val_loss: 1.7587 - val_acc: 0.6332\n",
            "Epoch 76/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7726 - acc: 0.6436 - val_loss: 1.7562 - val_acc: 0.6339\n",
            "Epoch 77/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7699 - acc: 0.6442 - val_loss: 1.7537 - val_acc: 0.6340\n",
            "Epoch 78/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7673 - acc: 0.6447 - val_loss: 1.7511 - val_acc: 0.6351\n",
            "Epoch 79/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7647 - acc: 0.6452 - val_loss: 1.7486 - val_acc: 0.6363\n",
            "Epoch 80/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7621 - acc: 0.6458 - val_loss: 1.7461 - val_acc: 0.6370\n",
            "Epoch 81/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7595 - acc: 0.6464 - val_loss: 1.7436 - val_acc: 0.6374\n",
            "Epoch 82/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7569 - acc: 0.6471 - val_loss: 1.7411 - val_acc: 0.6381\n",
            "Epoch 83/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7543 - acc: 0.6474 - val_loss: 1.7386 - val_acc: 0.6389\n",
            "Epoch 84/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7517 - acc: 0.6480 - val_loss: 1.7361 - val_acc: 0.6397\n",
            "Epoch 85/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7491 - acc: 0.6485 - val_loss: 1.7336 - val_acc: 0.6400\n",
            "Epoch 86/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7465 - acc: 0.6490 - val_loss: 1.7311 - val_acc: 0.6406\n",
            "Epoch 87/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7439 - acc: 0.6500 - val_loss: 1.7286 - val_acc: 0.6411\n",
            "Epoch 88/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7413 - acc: 0.6504 - val_loss: 1.7262 - val_acc: 0.6418\n",
            "Epoch 89/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7388 - acc: 0.6509 - val_loss: 1.7237 - val_acc: 0.6422\n",
            "Epoch 90/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7362 - acc: 0.6514 - val_loss: 1.7212 - val_acc: 0.6429\n",
            "Epoch 91/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7336 - acc: 0.6518 - val_loss: 1.7188 - val_acc: 0.6437\n",
            "Epoch 92/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7311 - acc: 0.6523 - val_loss: 1.7163 - val_acc: 0.6444\n",
            "Epoch 93/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7285 - acc: 0.6527 - val_loss: 1.7139 - val_acc: 0.6450\n",
            "Epoch 94/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7260 - acc: 0.6531 - val_loss: 1.7114 - val_acc: 0.6452\n",
            "Epoch 95/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7234 - acc: 0.6535 - val_loss: 1.7090 - val_acc: 0.6457\n",
            "Epoch 96/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7209 - acc: 0.6540 - val_loss: 1.7065 - val_acc: 0.6467\n",
            "Epoch 97/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7183 - acc: 0.6544 - val_loss: 1.7041 - val_acc: 0.6469\n",
            "Epoch 98/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7158 - acc: 0.6548 - val_loss: 1.7017 - val_acc: 0.6475\n",
            "Epoch 99/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7133 - acc: 0.6554 - val_loss: 1.6993 - val_acc: 0.6486\n",
            "Epoch 100/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7108 - acc: 0.6557 - val_loss: 1.6969 - val_acc: 0.6499\n",
            "Epoch 101/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7082 - acc: 0.6561 - val_loss: 1.6945 - val_acc: 0.6507\n",
            "Epoch 102/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7057 - acc: 0.6565 - val_loss: 1.6921 - val_acc: 0.6509\n",
            "Epoch 103/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.7032 - acc: 0.6569 - val_loss: 1.6897 - val_acc: 0.6514\n",
            "Epoch 104/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.7007 - acc: 0.6573 - val_loss: 1.6873 - val_acc: 0.6520\n",
            "Epoch 105/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6982 - acc: 0.6577 - val_loss: 1.6849 - val_acc: 0.6524\n",
            "Epoch 106/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6957 - acc: 0.6582 - val_loss: 1.6825 - val_acc: 0.6531\n",
            "Epoch 107/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6932 - acc: 0.6586 - val_loss: 1.6801 - val_acc: 0.6535\n",
            "Epoch 108/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6908 - acc: 0.6590 - val_loss: 1.6777 - val_acc: 0.6545\n",
            "Epoch 109/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6883 - acc: 0.6594 - val_loss: 1.6754 - val_acc: 0.6557\n",
            "Epoch 110/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6858 - acc: 0.6598 - val_loss: 1.6730 - val_acc: 0.6561\n",
            "Epoch 111/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6833 - acc: 0.6600 - val_loss: 1.6706 - val_acc: 0.6568\n",
            "Epoch 112/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6809 - acc: 0.6605 - val_loss: 1.6683 - val_acc: 0.6574\n",
            "Epoch 113/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6784 - acc: 0.6610 - val_loss: 1.6659 - val_acc: 0.6581\n",
            "Epoch 114/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6760 - acc: 0.6614 - val_loss: 1.6636 - val_acc: 0.6586\n",
            "Epoch 115/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6735 - acc: 0.6619 - val_loss: 1.6613 - val_acc: 0.6588\n",
            "Epoch 116/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6711 - acc: 0.6623 - val_loss: 1.6589 - val_acc: 0.6596\n",
            "Epoch 117/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6686 - acc: 0.6626 - val_loss: 1.6566 - val_acc: 0.6603\n",
            "Epoch 118/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6662 - acc: 0.6628 - val_loss: 1.6543 - val_acc: 0.6604\n",
            "Epoch 119/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6637 - acc: 0.6631 - val_loss: 1.6520 - val_acc: 0.6611\n",
            "Epoch 120/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6613 - acc: 0.6637 - val_loss: 1.6496 - val_acc: 0.6615\n",
            "Epoch 121/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6589 - acc: 0.6641 - val_loss: 1.6473 - val_acc: 0.6622\n",
            "Epoch 122/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6565 - acc: 0.6644 - val_loss: 1.6450 - val_acc: 0.6622\n",
            "Epoch 123/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6541 - acc: 0.6647 - val_loss: 1.6427 - val_acc: 0.6628\n",
            "Epoch 124/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6517 - acc: 0.6651 - val_loss: 1.6404 - val_acc: 0.6639\n",
            "Epoch 125/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6493 - acc: 0.6654 - val_loss: 1.6381 - val_acc: 0.6639\n",
            "Epoch 126/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6469 - acc: 0.6658 - val_loss: 1.6358 - val_acc: 0.6643\n",
            "Epoch 127/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6445 - acc: 0.6660 - val_loss: 1.6336 - val_acc: 0.6649\n",
            "Epoch 128/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6421 - acc: 0.6664 - val_loss: 1.6313 - val_acc: 0.6658\n",
            "Epoch 129/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6397 - acc: 0.6668 - val_loss: 1.6290 - val_acc: 0.6662\n",
            "Epoch 130/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6373 - acc: 0.6672 - val_loss: 1.6267 - val_acc: 0.6663\n",
            "Epoch 131/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6350 - acc: 0.6675 - val_loss: 1.6245 - val_acc: 0.6668\n",
            "Epoch 132/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6326 - acc: 0.6679 - val_loss: 1.6222 - val_acc: 0.6669\n",
            "Epoch 133/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6302 - acc: 0.6682 - val_loss: 1.6200 - val_acc: 0.6672\n",
            "Epoch 134/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6279 - acc: 0.6685 - val_loss: 1.6177 - val_acc: 0.6674\n",
            "Epoch 135/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6255 - acc: 0.6689 - val_loss: 1.6155 - val_acc: 0.6677\n",
            "Epoch 136/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6232 - acc: 0.6692 - val_loss: 1.6132 - val_acc: 0.6679\n",
            "Epoch 137/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6208 - acc: 0.6696 - val_loss: 1.6110 - val_acc: 0.6684\n",
            "Epoch 138/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6185 - acc: 0.6699 - val_loss: 1.6088 - val_acc: 0.6686\n",
            "Epoch 139/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6162 - acc: 0.6702 - val_loss: 1.6065 - val_acc: 0.6690\n",
            "Epoch 140/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6138 - acc: 0.6704 - val_loss: 1.6043 - val_acc: 0.6693\n",
            "Epoch 141/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6115 - acc: 0.6707 - val_loss: 1.6021 - val_acc: 0.6694\n",
            "Epoch 142/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6092 - acc: 0.6709 - val_loss: 1.5999 - val_acc: 0.6699\n",
            "Epoch 143/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.6069 - acc: 0.6711 - val_loss: 1.5977 - val_acc: 0.6706\n",
            "Epoch 144/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6046 - acc: 0.6713 - val_loss: 1.5955 - val_acc: 0.6711\n",
            "Epoch 145/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6023 - acc: 0.6715 - val_loss: 1.5933 - val_acc: 0.6712\n",
            "Epoch 146/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.6000 - acc: 0.6717 - val_loss: 1.5911 - val_acc: 0.6718\n",
            "Epoch 147/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5977 - acc: 0.6720 - val_loss: 1.5889 - val_acc: 0.6722\n",
            "Epoch 148/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5954 - acc: 0.6723 - val_loss: 1.5867 - val_acc: 0.6725\n",
            "Epoch 149/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5931 - acc: 0.6725 - val_loss: 1.5845 - val_acc: 0.6727\n",
            "Epoch 150/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5908 - acc: 0.6728 - val_loss: 1.5824 - val_acc: 0.6727\n",
            "Epoch 151/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5886 - acc: 0.6733 - val_loss: 1.5802 - val_acc: 0.6731\n",
            "Epoch 152/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5863 - acc: 0.6736 - val_loss: 1.5780 - val_acc: 0.6731\n",
            "Epoch 153/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5840 - acc: 0.6740 - val_loss: 1.5759 - val_acc: 0.6735\n",
            "Epoch 154/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5818 - acc: 0.6745 - val_loss: 1.5737 - val_acc: 0.6737\n",
            "Epoch 155/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5795 - acc: 0.6749 - val_loss: 1.5716 - val_acc: 0.6742\n",
            "Epoch 156/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5773 - acc: 0.6750 - val_loss: 1.5694 - val_acc: 0.6743\n",
            "Epoch 157/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5751 - acc: 0.6754 - val_loss: 1.5673 - val_acc: 0.6749\n",
            "Epoch 158/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5728 - acc: 0.6757 - val_loss: 1.5651 - val_acc: 0.6748\n",
            "Epoch 159/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5706 - acc: 0.6761 - val_loss: 1.5630 - val_acc: 0.6751\n",
            "Epoch 160/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5684 - acc: 0.6765 - val_loss: 1.5609 - val_acc: 0.6753\n",
            "Epoch 161/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5661 - acc: 0.6768 - val_loss: 1.5587 - val_acc: 0.6757\n",
            "Epoch 162/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5639 - acc: 0.6770 - val_loss: 1.5566 - val_acc: 0.6757\n",
            "Epoch 163/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5617 - acc: 0.6772 - val_loss: 1.5545 - val_acc: 0.6760\n",
            "Epoch 164/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5595 - acc: 0.6774 - val_loss: 1.5524 - val_acc: 0.6761\n",
            "Epoch 165/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5573 - acc: 0.6777 - val_loss: 1.5503 - val_acc: 0.6765\n",
            "Epoch 166/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5551 - acc: 0.6779 - val_loss: 1.5482 - val_acc: 0.6768\n",
            "Epoch 167/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5529 - acc: 0.6780 - val_loss: 1.5461 - val_acc: 0.6771\n",
            "Epoch 168/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5507 - acc: 0.6782 - val_loss: 1.5440 - val_acc: 0.6774\n",
            "Epoch 169/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5486 - acc: 0.6784 - val_loss: 1.5419 - val_acc: 0.6773\n",
            "Epoch 170/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5464 - acc: 0.6785 - val_loss: 1.5398 - val_acc: 0.6775\n",
            "Epoch 171/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5442 - acc: 0.6787 - val_loss: 1.5378 - val_acc: 0.6775\n",
            "Epoch 172/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5421 - acc: 0.6790 - val_loss: 1.5357 - val_acc: 0.6778\n",
            "Epoch 173/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5399 - acc: 0.6794 - val_loss: 1.5336 - val_acc: 0.6783\n",
            "Epoch 174/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5377 - acc: 0.6796 - val_loss: 1.5316 - val_acc: 0.6787\n",
            "Epoch 175/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5356 - acc: 0.6799 - val_loss: 1.5295 - val_acc: 0.6789\n",
            "Epoch 176/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5335 - acc: 0.6800 - val_loss: 1.5274 - val_acc: 0.6792\n",
            "Epoch 177/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5313 - acc: 0.6802 - val_loss: 1.5254 - val_acc: 0.6797\n",
            "Epoch 178/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5292 - acc: 0.6804 - val_loss: 1.5234 - val_acc: 0.6801\n",
            "Epoch 179/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5271 - acc: 0.6806 - val_loss: 1.5213 - val_acc: 0.6804\n",
            "Epoch 180/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5249 - acc: 0.6809 - val_loss: 1.5193 - val_acc: 0.6806\n",
            "Epoch 181/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5228 - acc: 0.6811 - val_loss: 1.5172 - val_acc: 0.6808\n",
            "Epoch 182/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5207 - acc: 0.6813 - val_loss: 1.5152 - val_acc: 0.6808\n",
            "Epoch 183/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5186 - acc: 0.6818 - val_loss: 1.5132 - val_acc: 0.6807\n",
            "Epoch 184/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5165 - acc: 0.6819 - val_loss: 1.5112 - val_acc: 0.6809\n",
            "Epoch 185/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5144 - acc: 0.6822 - val_loss: 1.5092 - val_acc: 0.6811\n",
            "Epoch 186/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5123 - acc: 0.6823 - val_loss: 1.5072 - val_acc: 0.6813\n",
            "Epoch 187/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5102 - acc: 0.6827 - val_loss: 1.5052 - val_acc: 0.6813\n",
            "Epoch 188/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5081 - acc: 0.6831 - val_loss: 1.5032 - val_acc: 0.6815\n",
            "Epoch 189/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5061 - acc: 0.6833 - val_loss: 1.5012 - val_acc: 0.6820\n",
            "Epoch 190/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.5040 - acc: 0.6836 - val_loss: 1.4992 - val_acc: 0.6819\n",
            "Epoch 191/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.5019 - acc: 0.6838 - val_loss: 1.4972 - val_acc: 0.6821\n",
            "Epoch 192/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.4999 - acc: 0.6841 - val_loss: 1.4952 - val_acc: 0.6824\n",
            "Epoch 193/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.4978 - acc: 0.6843 - val_loss: 1.4932 - val_acc: 0.6829\n",
            "Epoch 194/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.4958 - acc: 0.6847 - val_loss: 1.4913 - val_acc: 0.6831\n",
            "Epoch 195/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.4937 - acc: 0.6850 - val_loss: 1.4893 - val_acc: 0.6836\n",
            "Epoch 196/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.4917 - acc: 0.6852 - val_loss: 1.4873 - val_acc: 0.6835\n",
            "Epoch 197/200\n",
            "60000/60000 [==============================] - 1s 20us/sample - loss: 1.4896 - acc: 0.6854 - val_loss: 1.4854 - val_acc: 0.6837\n",
            "Epoch 198/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.4876 - acc: 0.6855 - val_loss: 1.4834 - val_acc: 0.6835\n",
            "Epoch 199/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.4856 - acc: 0.6856 - val_loss: 1.4815 - val_acc: 0.6835\n",
            "Epoch 200/200\n",
            "60000/60000 [==============================] - 1s 21us/sample - loss: 1.4836 - acc: 0.6858 - val_loss: 1.4795 - val_acc: 0.6838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffaeb0eb9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dr7Ev6mqm-Lh",
        "colab_type": "text"
      },
      "source": [
        "AS the number epochs are increased the validation accuracy starts doing up. from 100 to 200 leads to an increase in accuracy from 55% to 68%. Also it is not that the most efficient algorithm is the one with the most hidden layers.. we did achieve similar good accuracy in the first simple model as well. Thirdly each time we run the fit ; the accuracy changes due to random initiation of weights used."
      ]
    }
  ]
}