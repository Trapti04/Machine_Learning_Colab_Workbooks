{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-Project-1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreatLearningAIML1/delhi-may19-batch-Trapti04/blob/master/CNN_Project_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFu2KWASriRm",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "1.   Based on https://www.kaggle.com/c/plant-seedlings-classification problem\n",
        "2.   Objective : \n",
        "        Classify an image of seedling into one the following 12 different seedling classes.\n",
        "\n",
        "        Black-grass;\n",
        "Charlock;\n",
        "Cleavers;\n",
        "Common Chickweed;\n",
        "Common wheat;\n",
        "Fat Hen;\n",
        "Loose Silky-bent;\n",
        "Maize;\n",
        "Scentless Mayweed;\n",
        "Shepherds Purse;\n",
        "Small-flowered Cranesbill;\n",
        "Sugar beet\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jTpUQRcLcwB",
        "colab_type": "code",
        "outputId": "dd061875-6458-4b8e-9d2b-fe478dce167f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "!pip show tensorflow"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.15.0\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: opt-einsum, google-pasta, astor, absl-py, tensorboard, wrapt, numpy, gast, termcolor, grpcio, keras-preprocessing, six, keras-applications, tensorflow-estimator, wheel, protobuf\n",
            "Required-by: stable-baselines, magenta, fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQBGGTGkERul",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b92789b8-334c-4a55-f0c5-e60d42f7c8ac"
      },
      "source": [
        "!pip install --upgrade tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 66kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fc/08/8b927337b7019c374719145d1dceba21a8bb909b93b1ad6f8fb7d22c1ca1/tensorflow_estimator-2.0.1-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.6)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.17.4)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/54/99b9d5d52d5cb732f099baaaf7740403e83fe6b0cedde940fabd2b13d75a/tensorboard-2.0.2-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 26.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (42.0.2)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (2.21.0)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.1)\n",
            "Collecting google-auth<2,>=1.6.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f8/84b5771faec3eba9fe0c91c8c5896364a8ba08852c0dea5ad2025026dd95/google_auth-1.10.0-py2.py3-none-any.whl (76kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2019.11.28)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (2.8)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (4.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.2.7)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow) (3.1.0)\n",
            "\u001b[31mERROR: tensorboard 2.0.2 has requirement grpcio>=1.24.3, but you'll have grpcio 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement google-auth~=1.4.0, but you'll have google-auth 1.10.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator, google-auth, tensorboard, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "  Found existing installation: google-auth 1.4.2\n",
            "    Uninstalling google-auth-1.4.2:\n",
            "      Successfully uninstalled google-auth-1.4.2\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed google-auth-1.10.0 tensorboard-2.0.2 tensorflow-2.0.0 tensorflow-estimator-2.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xKMpCe-UFSIM",
        "colab_type": "code",
        "outputId": "5acb891f-f4f2-41db-ba87-7bd5b7c53947",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8f8LOXLNLNu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3LV1eQR1qwGb",
        "colab_type": "text"
      },
      "source": [
        "#Load libraries required"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iubfUNZBWssU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# System related\n",
        "import os\n",
        "import csv # for writing to file\n",
        "import itertools\n",
        "\n",
        "#Image related\n",
        "import cv2\n",
        "import tensorflow.keras.preprocessing.image \n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img,img_to_array\n",
        "\n",
        "# Data handling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Model building\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten, GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# visualization\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# metrics related\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4k9dOKgTUoC_",
        "colab_type": "text"
      },
      "source": [
        "#Description:\n",
        "The Plant Seedlings Dataset contains images of approximately 960 unique plants belonging to 12 species at several growth stages.It comprises annotated RGB images with a physical resolution of roughly 10 pixels per mm.\n",
        "\n",
        "# Evaluation criteria\n",
        "Submissions are evaluated on MeanFScore,\n",
        "which at Kaggle is actually a micro-averaged F1-score.\n",
        "\n",
        "Given positive/negative rates for each class k, the resulting score is computed this way:\n",
        "$$Precision{micro} = \\frac{\\sum{k \\in C} TPk}{\\sum{k \\in C} TPk + FPk}\n",
        "\n",
        "Recall{micro} = \\frac{\\sum{k \\in C} TPk}{\\sum{k \\in C} TPk + FNk}\n",
        "\n",
        "F1−score is the harmonic mean of precision and recall\n",
        "MeanFScore = F1{micro}= \\frac{2 Precision{micro} Recall{micro}}{Precision{micro} + Recall_{micro}}$$.\n",
        "\n",
        "fro wikipedia: The harmonic mean of the precision (true positives per predicted positive) and the recall (true positives per real positive) is often used as an aggregated performance score for the evaluation of algorithms and systems: the F-score (or F-measure). This is used in information retrieval because only the positive class is of relevance, while number of negatives, in general, is large and unknown.[12] It is thus a trade-off as to whether the correct positive predictions should be measured in relation to the number of predicted positives or the number of real positives, so it is measured versus a putative number of positives that is an arithmetic mean of the two possible denominators.\n",
        "\n",
        "# Steps to solve\n",
        "Step 1: load the data from my google drive- observe the dataset for imbalance ; nature of pictures to see what feature engineering might be required."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JuLfFH6OORy",
        "colab_type": "code",
        "outputId": "25466885-8591-49c4-e8e5-5f831fe0cb8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "%ls \"/content/drive/My Drive/Computer vision/Project1-CNN/train\"\n",
        " "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \u001b[0m\u001b[01;34mBlack-grass\u001b[0m/        \u001b[01;34m'Common wheat'\u001b[0m/      \u001b[01;34m'Scentless Mayweed'\u001b[0m/\n",
            " \u001b[01;34mCharlock\u001b[0m/           \u001b[01;34m'Fat Hen'\u001b[0m/           \u001b[01;34m'Shepherds Purse'\u001b[0m/\n",
            " \u001b[01;34mCleavers\u001b[0m/           \u001b[01;34m'Loose Silky-bent'\u001b[0m/  \u001b[01;34m'Small-flowered Cranesbill'\u001b[0m/\n",
            "\u001b[01;34m'Common Chickweed'\u001b[0m/   \u001b[01;34mMaize\u001b[0m/              \u001b[01;34m'Sugar beet'\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeEvMYUvl8sA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def classes_to_int(label):\n",
        "    label = label.strip()\n",
        "    if label == \"Black-grass\":  return 0\n",
        "    if label == \"Charlock\":  return 1\n",
        "    if label == \"Cleavers\":  return 2\n",
        "    if label == \"Common Chickweed\":  return 3\n",
        "    if label == \"Common wheat\":  return 4\n",
        "    if label == \"Fat Hen\":  return 5\n",
        "    if label == \"Loose Silky-bent\": return 6\n",
        "    if label == \"Maize\":  return 7\n",
        "    if label == \"Scentless Mayweed\": return 8\n",
        "    if label == \"Shepherds Purse\": return 9\n",
        "    if label == \"Small-flowered Cranesbill\": return 10\n",
        "    if label == \"Sugar beet\": return 11\n",
        "    print(\"Invalid Label\", label)\n",
        "    return 12\n",
        "\n",
        "def int_to_classes(i):\n",
        "    if i == 0: return \"Black-grass\"\n",
        "    elif i == 1: return \"Charlock\"\n",
        "    elif i == 2: return \"Cleavers\"\n",
        "    elif i == 3: return \"Common Chickweed\"\n",
        "    elif i == 4: return \"Common wheat\"\n",
        "    elif i == 5: return \"Fat Hen\"\n",
        "    elif i == 6: return \"Loose Silky-bent\"\n",
        "    elif i == 7: return \"Maize\"\n",
        "    elif i == 8: return \"Scentless Mayweed\"\n",
        "    elif i == 9: return \"Shepherds Purse\"\n",
        "    elif i == 10: return \"Small-flowered Cranesbill\"\n",
        "    elif i == 11: return \"Sugar beet\"\n",
        "    print(\"Invalid class \", i)\n",
        "    return \"Invalid Class\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3gbxUUChpif",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HEIGHT = 128\n",
        "WIDTH = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4Njmd5FQUMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readTrainData(trainDir):\n",
        "    data = []\n",
        "    labels = []\n",
        "    \n",
        "    # loop over the input images\n",
        "    dirs = os.listdir(trainDir) \n",
        "    for dir in dirs:\n",
        "        absDirPath = os.path.join(os.path.sep,trainDir, dir)\n",
        "        images = os.listdir(absDirPath)\n",
        "        for imageFileName in images:\n",
        "            # load the image, pre-process it, and store it in the data list\n",
        "            imageFullPath = os.path.join(trainDir, dir, imageFileName)\n",
        "            #print(imageFullPath)\n",
        "            img = load_img(imageFullPath)\n",
        "            arr = img_to_array(img)  # Numpy array with shape (233,233,3)\n",
        "            arr = cv2.resize(arr, (HEIGHT,WIDTH)) #Numpy array with shape (HEIGHT, WIDTH,3)\n",
        "            #print(arr.shape) \n",
        "            data.append(arr)\n",
        "            label = classes_to_int(dir)\n",
        "            labels.append(label)\n",
        "    return data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53aOEOEwitzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, Y = readTrainData(\"/content/drive/My Drive/Computer vision/Project1-CNN/train\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NZzGO5991dN",
        "colab_type": "code",
        "outputId": "200bc369-3c92-4eb5-97bd-f7cff0ad3a53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "#see number of images in each label\n",
        "images = np.array(X)\n",
        "#Y_class = np.array([np.argmax(y, axis=None, out=None) for y in Y ])\n",
        "classes = np.array(Y)\n",
        "#print(\"images shape: \", X.shape)\n",
        "#print(\"classes shape: \", Y_class.shape)\n",
        "#for key,value in images_per_class.items():\n",
        "   # print(\"number of {0} images is  -> {1}\".format(key, len(value)))\n",
        "  \n",
        "for label in set(classes):\n",
        "\n",
        "  print(\"number of {} images is  -> {}\".format(\n",
        "        label, \n",
        "        len(images[classes == label])))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of 0 images is  -> 263\n",
            "number of 1 images is  -> 390\n",
            "number of 2 images is  -> 287\n",
            "number of 3 images is  -> 611\n",
            "number of 4 images is  -> 221\n",
            "number of 5 images is  -> 475\n",
            "number of 6 images is  -> 654\n",
            "number of 7 images is  -> 221\n",
            "number of 8 images is  -> 516\n",
            "number of 9 images is  -> 231\n",
            "number of 10 images is  -> 496\n",
            "number of 11 images is  -> 385\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FBNw4FObFDXh",
        "colab_type": "code",
        "outputId": "87da7941-d3e6-4b54-b38f-e02b7ef125f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "values =[]\n",
        "labels=[]\n",
        "for label in set(classes):\n",
        "  values.append(len(images[classes == label]))\n",
        "  labels.append(label)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(12,7))\n",
        "ax.bar(labels, values)\n",
        "ax.set_xlabel(\" classes\")\n",
        "ax.set_ylabel(\"number of images\")\n",
        "#axs[1].scatter(label, )\n",
        "  #axs[2].plot(names, values)\n",
        "fig.suptitle('Categorical Plotting')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 0.98, 'Categorical Plotting')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtMAAAHcCAYAAAAUSiZ5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2XTKqKHIe6i",
        "colab_type": "text"
      },
      "source": [
        "Step 2: The classes are imbalanced and we need to see if we can under/ over sample to get over the imbalance  if it can deliver better accuracy. we will also  see what other improvements can be made to get better classification basis initial confustion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw4OIkkqs_vz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = np.array(X, dtype=\"float\") / 255.0\n",
        "Y = np.array(Y)\n",
        "# convert the labels from integers to vectors\n",
        "Y =  tensorflow.keras.utils.to_categorical(Y, num_classes=12)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RViUUfQDFU3b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# partition the data into training and testing splits using 75% training and 25% for validation\n",
        "(trainX, valX, trainY, valY) = train_test_split(X,Y,test_size=0.25, random_state= 70)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi3cyAM8Gwwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#construct the image generator for data augmentation\n",
        "\n",
        "aug = ImageDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        shear_range=0.2,\n",
        "        zoom_range = 0.2, # Randomly zoom image \n",
        "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=True,  # randomly flip images\n",
        "        vertical_flip=False,  # randomly flip images\n",
        "        fill_mode =\"nearest\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbMjviBPRHT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NUM_CLASSES = 12\n",
        "# we need images of same size so we convert them into the size\n",
        "DEPTH = 3\n",
        "inputShape = (WIDTH, HEIGHT, DEPTH)\n",
        "# initialize number of epochs to train for, initial learning rate and batch size\n",
        "EPOCHS = 40\n",
        "INIT_LR = 1e-3\n",
        "BS = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_BhTe9kG3mr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createModel():\n",
        "    model = Sequential()\n",
        "    # first set of CONV => RELU => POOL layers\n",
        "    # The CONV  layer will learn 32 convolution filters, each of which are 5×5.\n",
        "    model.add(Conv2D(32, (5, 5), padding=\"same\",activation = \"relu\", input_shape=inputShape))\n",
        "      \n",
        "    # second set of CONV => RELU => POOL layers\n",
        "    model.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "    model.add(Dropout(0.2))\n",
        "    # third set of CONV => RELU => POOL layers\n",
        "    #This time we are learning 64 convolutional filters rather than the 32 convolutional\n",
        "    #filters as in the previous layer set. It’s common to see the number of CONV \n",
        "    #filters learned increase the deeper we go in the network architecture.\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'same', \n",
        "                 activation ='relu'))\n",
        "    model.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'same', \n",
        "                 activation ='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "    model.add(Dropout(0.3))\n",
        "    \n",
        "    # 4th layer\n",
        " \n",
        "    model.add(Conv2D(filters = 128, kernel_size = (3,3),padding = 'Same', \n",
        "                 activation ='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
        "    model.add(Dropout(0.4))\n",
        "    # Flattening out the volume into a set of fully-connected layers\n",
        "    # Take the output of the preceding MaxPooling2D layer and flatten it into a single vector.\n",
        "    # This operation allows us to apply our dense/fully-connected layers.\n",
        "    # Fully-connected layer contains 256 nodes which is passed through another \n",
        "    # nonlinear ReLU activation.\n",
        "    model.add(GlobalMaxPooling2D())\n",
        "    model.add(Flatten())\n",
        "    \n",
        "    model.add(Dense(300, activation = \"relu\"))\n",
        "    model.add(Dropout(0.5))\n",
        "    # softmax classifier\n",
        "    # Another fully-connected layer, but this one is special — the number of nodes is equal \n",
        "    # to the number of classes  (i.e., the classes we want to recognize).\n",
        "    # This Dense layer is then fed into our softmax classifier\n",
        "    # which will yield the probability for each class.\n",
        "    model.add(Dense(NUM_CLASSES, activation = \"softmax\"))\n",
        "    model.summary()\n",
        "    # returns our fully constructed deep learning + Keras image classifier \n",
        "    #opt = Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS)\n",
        "    opt = Adam(lr=INIT_LR)\n",
        "    # use binary_crossentropy if there are two classes\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC6Ce5RxeEee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', \n",
        "                                            patience=3, \n",
        "                                            verbose=1, \n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMmjb7hPHYW2",
        "colab_type": "code",
        "outputId": "5f228d88-b64f-4d88-a03d-e1d0de0dcd82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "\n",
        "\n",
        "# initialize the model\n",
        "print(\"compiling model...\")\n",
        "#sys.stdout.flush()\n",
        "model = createModel()\n",
        "# train the network\n",
        "print(\"training network...\")\n",
        "#sys.stdout.flush()\n",
        "H = model.fit_generator(aug.flow(trainX, trainY, batch_size=BS), \\\n",
        "    validation_data=(valX, valY), \\\n",
        "    steps_per_epoch=len(trainX) // BS, epochs=EPOCHS, verbose=1,\\\n",
        "    callbacks = [learning_rate_reduction])\n",
        "# callbacks = [learning_rate_reduction]\n",
        "# save the model to disk\n",
        "print(\"Saving model to disk\")\n",
        "#sys.stdout.flush()\n",
        "model.save(\"/tmp/1_model\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "compiling model...\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_5 (Conv2D)            (None, 128, 128, 32)      2432      \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 128, 128, 32)      25632     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 64, 64, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 64, 64, 64)        18496     \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 64, 64, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 32, 32, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 32, 32, 128)       73856     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 16, 16, 128)       0         \n",
            "_________________________________________________________________\n",
            "global_max_pooling2d_1 (Glob (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 300)               38700     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 12)                3612      \n",
            "=================================================================\n",
            "Total params: 199,656\n",
            "Trainable params: 199,656\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "training network...\n",
            "Epoch 1/40\n",
            " 33/111 [=======>......................] - ETA: 8:05 - loss: 2.4275 - accuracy: 0.1316"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iezP8jvpiMpf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo4XSWqHnllt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the matplotlib backend so figures can be saved in the background\n",
        "# plot the training loss and accuracy\n",
        "print(\"Generating plots...\")\n",
        "\n",
        "matplotlib.use(\"Agg\")\n",
        "matplotlib.pyplot.style.use(\"ggplot\")\n",
        "matplotlib.pyplot.figure()\n",
        "N = EPOCHS\n",
        "matplotlib.pyplot.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
        "matplotlib.pyplot.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
        "matplotlib.pyplot.plot(np.arange(0, N), H.history[\"accuracy\"], label=\"train_acc\")\n",
        "matplotlib.pyplot.plot(np.arange(0, N), H.history[\"val_accuracy\"], label=\"val_acc\")\n",
        "matplotlib.pyplot.title(\"Training Loss and Accuracy on  crop classification\")\n",
        "matplotlib.pyplot.xlabel(\"Epoch #\")\n",
        "matplotlib.pyplot.ylabel(\"Loss/Accuracy\")\n",
        "matplotlib.pyplot.legend(loc=\"lower left\")\n",
        "matplotlib.pyplot.savefig(\"plot.png\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frUtaSyd8umt",
        "colab_type": "text"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvsIEMXz7-JT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "fig, ax = plt.subplots(2,1 , figsize=(20,7))\n",
        "ax[0].plot(H.history['loss'], color='b', label=\"Training loss\")\n",
        "ax[0].plot(H.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\n",
        "legend = ax[0].legend(loc='best', shadow=True)\n",
        "\n",
        "ax[1].plot(H.history['accuracy'], color='b', label=\"Training accuracy\")\n",
        "ax[1].plot(H.history['val_accuracy'], color='r',label=\"Validation accuracy\")\n",
        "legend = ax[1].legend(loc='best', shadow=True)\n",
        "plt.savefig(\"loss-acc-plot.png\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anv4eP2uoQ3S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def readTestData(testDir):\n",
        "    data = []\n",
        "    filenames = []\n",
        "    # loop over the input images\n",
        "    images = os.listdir(testDir)\n",
        "    for imageFileName in images:\n",
        "        # load the image, pre-process it, and store it in the data list\n",
        "        imageFullPath = os.path.join(testDir, imageFileName)\n",
        "        #print(imageFullPath)\n",
        "        img = load_img(imageFullPath)\n",
        "        arr = img_to_array(img)  # Numpy array with shape (...,..,3)\n",
        "        arr = cv2.resize(arr, (HEIGHT,WIDTH)) \n",
        "        data.append(arr)\n",
        "        filenames.append(imageFileName)\n",
        "    return data, filenames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bcl-qCEjonoY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read test data and find its classification\n",
        "testX, filenames = readTestData(\"/content/drive/My Drive/Computer vision/Project1-CNN/test\")\n",
        "# scale the raw pixel intensities to the range [0, 1]\n",
        "testX = np.array(testX, dtype=\"float\") / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfeWl37rpOGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "mymodel = load_model('/tmp/1_model')\n",
        "yFit = mymodel.predict(testX, batch_size=10, verbose=1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibHFZ4zlpkLk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " \n",
        "with open('output.csv', 'w', newline='') as csvfile:\n",
        "    fieldnames = ['file', 'species']\n",
        "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "    writer.writeheader()\n",
        "    for index, file in enumerate(filenames):\n",
        "        classesProbs = yFit[index]\n",
        "        maxIdx = 0\n",
        "        maxProb = 0;\n",
        "        for idx in range(0,11):\n",
        "            if(classesProbs[idx] > maxProb):\n",
        "                maxIdx = idx\n",
        "                maxProb = classesProbs[idx]\n",
        "        writer.writerow({'file': file, 'species': int_to_classes(maxIdx)})\n",
        "print(\"Writing complete\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3AO5ItV3r-FL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "plt.subplots(figsize=(22,7)) #set the size of the plot \n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize= False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.savefig(\"confusion_matrix.png\")\n",
        "\n",
        "\n",
        "# Predict the values from the validation dataset\n",
        "Y_pred = model.predict(valX)\n",
        "# Convert predictions classes to one hot vectors \n",
        "Y_pred_classes = np.argmax(Y_pred ,axis = 1) \n",
        "# Convert validation observations to one hot vectors\n",
        "Y_true = np.argmax(valY,axis = 1) \n",
        "# compute the confusion matrix\n",
        "\n",
        "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n",
        "# plot the confusion matrix\n",
        "plot_confusion_matrix(confusion_mtx, classes = range(NUM_CLASSES))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mc5HKAvTQ190",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "target_names = ['class 0', 'class 1', 'class 2', 'class 3', 'class 4', 'class 5', 'class 6', 'class 7', 'class 8', 'class 9', 'class 10', 'class 11']\n",
        "print(classification_report(Y_true, Y_pred_classes, target_names=target_names))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nn-NKTFCLhME",
        "colab_type": "text"
      },
      "source": [
        "Observations on various runs:\n",
        "\n",
        "Itr 1: used RGB and 128 by 128 pixel images, no feature egineering. model used was basic with 2 Conv2D and 2 max pooling layers. accuracy achieved  0.77 with Lr 0.001 and decay as 0.001/15[15= number of epochs]\n",
        "\n",
        "Itr 2; repeating with same  but epochs are 30. accuracy increases to 0.7980. F1 score = 0.80\n",
        "\n",
        "Itr3: Learning rate reduction on Plateau and epochs 50. F1 accuarcy increased to 0.87\n",
        "\n",
        "Itr4: Image resize has 256 * 256 rather than 128 * 128 - result errors out due to RAm limitation with TPU on colab.  errors out at 200 * 200 as well. Hence moving it back to 128 * 128, only item introduced was reducing learnng rate on plateau and no. of epochs reduced to 40. F1 accuarcy was 0.79.\n",
        "Itr5: same as Itr4 but with the last Dense layer having 500 neurons instead of 256.\n",
        "\n",
        "itr 5:  modified the model definition to include 2 additional Conv2D layers with more drop outs at each layer. no of epochs are 40\n",
        "\n",
        "itr6: With prepocessing of images\n",
        "\n",
        "\n"
      ]
    }
  ]
}