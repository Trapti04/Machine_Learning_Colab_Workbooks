{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Trapti04/Machine_Learning_Colab_Workbooks/blob/master/CIFAR_dataset__deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uO7a9Qrfjl5"
      },
      "source": [
        "# Classifying images of everyday objects using a neural network\n",
        "\n",
        "The ability to try many different neural network architectures to address a problem is what makes deep learning really powerful, especially compared to shallow learning techniques like linear regression, logistic regression etc. \n",
        "\n",
        "In this assignment, you will:\n",
        "\n",
        "1. Explore the CIFAR10 dataset: https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "2. Set up a training pipeline to train a neural network on a GPU\n",
        "2. Experiment with different network architectures & hyperparameters\n",
        "\n",
        "As you go through this notebook, you will find a **???** in certain places. Your job is to replace the **???** with appropriate code or values, to ensure that the notebook runs properly end-to-end. Try to experiment with different network structures and hypeparameters to get the lowest loss.\n",
        "\n",
        "You might find these notebooks useful for reference, as you work through this notebook:\n",
        "\n",
        "- https://jovian.ml/aakashns/04-feedforward-nn\n",
        "- https://jovian.ml/aakashns/fashion-feedforward-minimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nwl-i01cfjmH"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run the commands below if imports fail\n",
        "# !conda install numpy pandas pytorch torchvision cpuonly -c pytorch -y\n",
        "# !pip install matplotlib --upgrade --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "jtbVzM72fjmN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.utils import make_grid\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from torch.utils.data import random_split\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cfCtH9xfjmX"
      },
      "source": [
        "## Exploring the CIFAR10 dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "id": "E07_GPs-fjmc",
        "outputId": "c737b009-17c3-4119-ef3e-a7cc4a7748eb"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-8abcd792b819>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdownload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_integrity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Files already downloaded and verified\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mdownload_and_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtgz_md5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_and_extract_archive\u001b[0;34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m     \u001b[0mdownload_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmd5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_root\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36mdownload_url\u001b[0;34m(url, root, filename, md5, max_redirect_hops)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# expand redirect chain if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_redirect_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_hops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_redirect_hops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m# check if file is located on Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\u001b[0m in \u001b[0;36m_get_redirect_url\u001b[0;34m(url, max_hops)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_hops\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mprocessor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_response\u001b[0;34m(self, request, response)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mcode\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             response = self.parent.error(\n\u001b[0;32m--> 641\u001b[0;31m                 'http', request, response, code, msg, hdrs)\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36merror\u001b[0;34m(self, proto, *args)\u001b[0m\n\u001b[1;32m    567\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_err\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'default'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'http_error_default'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0morig_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 569\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_chain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[0;31m# XXX probably also want an abstract factory that knows when it makes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    501\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhandler\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhandlers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 503\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    504\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mhttp_error_default\u001b[0;34m(self, req, fp, code, msg, hdrs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPDefaultErrorHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mhttp_error_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhdrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mHTTPRedirectHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseHandler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: HTTP Error 503: Service Unavailable"
          ]
        }
      ],
      "source": [
        "dataset = CIFAR10(root='data/', download=True, transform=ToTensor())\n",
        "test_dataset = CIFAR10(root='data/', train=False, transform=ToTensor())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBpvf_fyfjmn"
      },
      "source": [
        "**Q: How many images does the training dataset contain?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "QNmXeilsfjms",
        "outputId": "a7ac64e8-800d-401d-9678-0cea0a3f9a8b"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ad797b803e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ],
      "source": [
        "dataset_size = len(dataset)\n",
        "dataset_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVf4ehrJfjmz"
      },
      "source": [
        "**Q: How many images does the test dataset contain?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aq5ZN0vfjm2"
      },
      "outputs": [],
      "source": [
        "test_dataset_size = len(test_dataset)\n",
        "test_dataset_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aY96EXGwfjm6"
      },
      "source": [
        "**Q: How many output classes does the dataset contain? Can you list them?**\n",
        "\n",
        "Hint: Use `dataset.classes`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zk7HRw7ffjm9"
      },
      "outputs": [],
      "source": [
        "classes = dataset.classes\n",
        "classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pCyjOdnAfjnA"
      },
      "outputs": [],
      "source": [
        "num_classes = len(dataset.classes)\n",
        "num_classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5apYBMH1fjnD"
      },
      "source": [
        "**Q: What is the shape of an image tensor from the dataset?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5FLaViwSfjnF"
      },
      "outputs": [],
      "source": [
        "img, label = dataset[0]\n",
        "img_shape = img.shape\n",
        "img_shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Br-zxJk5fjnH"
      },
      "source": [
        "Note that this dataset consists of 3-channel color images (RGB). Let us look at a sample image from the dataset. `matplotlib` expects channels to be the last dimension of the image tensors (whereas in PyTorch they are the first dimension), so we'll the `.permute` tensor method to shift channels to the last dimension. Let's also print the label for the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LHvTH4o-fjnK"
      },
      "outputs": [],
      "source": [
        "img, label = dataset[0]\n",
        "plt.imshow(img.permute((1, 2, 0)))\n",
        "print('Label (numeric):', label)\n",
        "print('Label (textual):', classes[label])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BES5pbD-fjnM"
      },
      "source": [
        "**(Optional) Q: Can you determine the number of images belonging to each class?**\n",
        "\n",
        "Hint: Loop through the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zorUnVt9fjnP"
      },
      "outputs": [],
      "source": [
        "# declar a map to store class name as key and value as total class count\n",
        "d = {}\n",
        " # enter a new key for each new lbl and update is value  with a count\n",
        " # if the key already exists then just incremet the counter\n",
        "for img,lbl in dataset:\n",
        "    if lbl not in d.keys():\n",
        "        d[lbl] = 0\n",
        "    d[lbl] += 1\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d"
      ],
      "metadata": {
        "id": "WFEBzdYc_AJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdd4Ty73fjnd"
      },
      "source": [
        "## Preparing the data for training\n",
        "\n",
        "We'll use a validation set with 5000 images (10% of the dataset). To ensure we get the same validation set each time, we'll set PyTorch's random number generator to a seed value of 43."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zOZxrJafjnf"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(43)\n",
        "val_size = 5000\n",
        "train_size = len(dataset) - val_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iT2E0xCrfjnh"
      },
      "source": [
        "Let's use the `random_split` method to create the training & validation sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEoZNpG7fjni"
      },
      "outputs": [],
      "source": [
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "len(train_ds), len(val_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bz2A_xUfjnl"
      },
      "source": [
        "We can now create data loaders to load the data in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfviFOxyfjnm"
      },
      "outputs": [],
      "source": [
        "batch_size= 128 #max accuracy .53 with 128, max accuracy .52 with 64, sdg, 20,20,20 epochs of 0.1,0.5,0.01 lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYrsj41tfjno"
      },
      "outputs": [],
      "source": [
        "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size*2, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size*2, num_workers=2, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URarFIiefjnp"
      },
      "source": [
        "Let's visualize a batch of data using the `make_grid` helper function from Torchvision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMFQYCr9fjnq"
      },
      "outputs": [],
      "source": [
        "for images, _ in train_loader:\n",
        "    print('images.shape:', images.shape)\n",
        "    plt.figure(figsize=(16,8))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(make_grid(images, nrow=16).permute((1, 2, 0)))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLMEASO4fjnt"
      },
      "source": [
        "Can you label all the images by looking at them? Trying to label a random sample of the data manually is a good way to estimate the difficulty of the problem, and identify errors in labeling, if any."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zXMi8D8mfjnv"
      },
      "source": [
        "## Base Model class & Training on GPU\n",
        "\n",
        "Let's create a base model class, which contains everything except the model architecture i.e. it wil not contain the `__init__` and `__forward__` methods. We will later extend this class to try out different architectures. In fact, you can extend this model to solve any image classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1AJfKzkfjnw"
      },
      "outputs": [],
      "source": [
        "def accuracy(outputs, labels):\n",
        "    _, preds = torch.max(outputs, dim=1) # first output is the max value and second is the index of max value. We are interested in index position of max value.\n",
        "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-81ZiMjfjny"
      },
      "outputs": [],
      "source": [
        "class ImageClassificationBase(nn.Module):\n",
        "    def training_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                  # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch):\n",
        "        images, labels = batch \n",
        "        out = self(images)                    # Generate predictions\n",
        "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
        "        acc = accuracy(out, labels)           # Calculate accuracy\n",
        "        return {'val_loss': loss.detach(), 'val_acc': acc} # https://stackoverflow.com/questions/56816241/difference-between-detach-and-with-torch-nograd-in-pytorch\n",
        "        \n",
        "    def validation_epoch_end(self, outputs):\n",
        "        batch_losses = [x['val_loss'] for x in outputs]\n",
        "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
        "        batch_accs = [x['val_acc'] for x in outputs]\n",
        "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies # https://stackoverflow.com/questions/69220221/use-of-torch-stack for diff between stack and contactenation\n",
        "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
        "    \n",
        "    def epoch_end(self, epoch, result):\n",
        "        print(\"Epoch [{}], val_loss: {:.4f}, val_acc: {:.4f}\".format(epoch, result['val_loss'], result['val_acc']))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# showing difference between stacking and concat. Not  in the stack function they expect tuples  or more of tensor objects. Stacking adds a dim; concat is along one of the dim.\n",
        "a_list = [3,4,5,6,7] \n",
        "b_list = [10,9,8,7,6]\n",
        "a_tensor = torch.Tensor(a_list)\n",
        "b_tensor = torch.Tensor(b_list)\n",
        "l_stacked = torch.stack((a_tensor, b_tensor))\n",
        "l_concat = torch.concat((a_tensor,b_tensor))\n",
        "l_stacked"
      ],
      "metadata": {
        "id": "qOmBKUJhZCAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_concat"
      ],
      "metadata": {
        "id": "ryhuN62uZZxg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTPNeaSkfjn1"
      },
      "source": [
        "We can also use the exact same training loop as before. I hope you're starting to see the benefits of refactoring our code into reusable functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60TCSOlwfjn2"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, val_loader):\n",
        "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
        "    return model.validation_epoch_end(outputs)\n",
        "\n",
        "def fit(epochs, lr, model, train_loader, val_loader, opt_func=torch.optim.SGD): # torch.optim.SGD= .50, ASGD= .49. accuracy,torch.optim.RMSprop,torch.optim.Adam= no learning\n",
        "    history = []\n",
        "    optimizer = opt_func(model.parameters(), lr)\n",
        "    for epoch in range(epochs):\n",
        "        # Training Phase \n",
        "        for batch in train_loader:\n",
        "            loss = model.training_step(batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "        # Validation phase\n",
        "        result = evaluate(model, val_loader)\n",
        "        model.epoch_end(epoch, result)\n",
        "        history.append(result)\n",
        "    return history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKCbFWRVfjn3"
      },
      "source": [
        "Finally, let's also define some utilities for moving out data & labels to the GPU, if one is available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upnDLa1Rfjos"
      },
      "outputs": [],
      "source": [
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mm4YPM7rfjot"
      },
      "outputs": [],
      "source": [
        "def get_default_device():\n",
        "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        return torch.device('cuda')\n",
        "    else:\n",
        "        return torch.device('cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQgBp8w7fjou"
      },
      "outputs": [],
      "source": [
        "device = get_default_device()\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zutaZE92fjov"
      },
      "outputs": [],
      "source": [
        "def to_device(data, device):\n",
        "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
        "    if isinstance(data, (list,tuple)):\n",
        "        return [to_device(x, device) for x in data]\n",
        "    return data.to(device, non_blocking=True)\n",
        "\n",
        "class DeviceDataLoader():\n",
        "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
        "    def __init__(self, dl, device):\n",
        "        self.dl = dl\n",
        "        self.device = device\n",
        "        \n",
        "    def __iter__(self):\n",
        "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
        "        for b in self.dl: \n",
        "            yield to_device(b, self.device)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Number of batches\"\"\"\n",
        "        return len(self.dl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKPY6vSmfjox"
      },
      "source": [
        "Let us also define a couple of helper functions for plotting the losses & accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxFqIcVqfjo0"
      },
      "outputs": [],
      "source": [
        "def plot_losses(history):\n",
        "    losses = [x['val_loss'] for x in history]\n",
        "    plt.plot(losses, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('loss')\n",
        "    plt.title('Loss vs. No. of epochs');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gNKLHe8fjo1"
      },
      "outputs": [],
      "source": [
        "def plot_accuracies(history):\n",
        "    accuracies = [x['val_acc'] for x in history]\n",
        "    plt.plot(accuracies, '-x')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.title('Accuracy vs. No. of epochs');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "504IguPXfjo2"
      },
      "source": [
        "Let's move our data loaders to the appropriate device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7eFiK-Mfjo3"
      },
      "outputs": [],
      "source": [
        "train_loader = DeviceDataLoader(train_loader, device)\n",
        "val_loader = DeviceDataLoader(val_loader, device)\n",
        "test_loader = DeviceDataLoader(test_loader, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WinRfyqTfjo4"
      },
      "source": [
        "## Training the model\n",
        "\n",
        "We will make several attempts at training the model. Each time, try a different architecture and a different set of learning rates. Here are some ideas to try:\n",
        "- Increase or decrease the number of hidden layers\n",
        "- Increase of decrease the size of each hidden layer\n",
        "- Try different activation functions\n",
        "- Try training for different number of epochs\n",
        "- Try different learning rates in every epoch\n",
        "\n",
        "What's the highest validation accuracy you can get to? **Can you get to 50% accuracy? What about 60%?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbVbm5wOfjo4"
      },
      "outputs": [],
      "source": [
        "in_size = 3*32*32\n",
        "out_size = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o57dR5Ffjo5"
      },
      "source": [
        "\n",
        "**Q: Extend the `ImageClassificationBase` class to complete the model definition.**\n",
        "\n",
        "Hint: Define the `__init__` and `forward` methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvLCmNWXfjo5"
      },
      "outputs": [],
      "source": [
        "class CIFAR10Model(ImageClassificationBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ???\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        # Flatten images into vectors\n",
        "        out = xb.view(xb.size(0), -1)\n",
        "        # Apply layers & activation functions\n",
        "        ???\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size_1 = 128\n",
        "hidden_size_2 = 64"
      ],
      "metadata": {
        "id": "_bOItPiO0o3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10Model(ImageClassificationBase):\n",
        "    \"\"\"Feedfoward neural network with 2 hidden layer\"\"\"\n",
        "    def __init__(self, in_size, hidden_size_1, hidden_size_2, out_size):\n",
        "        super().__init__()\n",
        "        # hidden layer 1\n",
        "        self.linear1 = nn.Linear(in_size, hidden_size_1)\n",
        "        # introducing drop out for regularization\n",
        "        self.reg1 = nn.Dropout(p= 0.2)\n",
        "        # hidden layer 2\n",
        "        self.linear2 = nn.Linear(hidden_size_1, hidden_size_2)\n",
        "        #reg layer\n",
        "        self.reg2 = nn.Dropout(p= 0.2)\n",
        "        # output layer\n",
        "        self.linear3 = nn.Linear(hidden_size_2, out_size)\n",
        "        \n",
        "    def forward(self, xb):\n",
        "        # Flatten the image tensors\n",
        "        xb = xb.view(xb.size(0), -1)\n",
        "        # Get intermediate outputs using hidden layer\n",
        "        out = self.linear1(xb)\n",
        "        # Apply activation function\n",
        "        out = F.relu(out)\n",
        "        #apply dropout\n",
        "        out = self.reg1(out)\n",
        "        #Get intermediate outputs froom next hidden layer\n",
        "        out = self.linear2(out)\n",
        "        # Apply activation function\n",
        "        out = F.relu(out)\n",
        "        #apply dropout\n",
        "        out = self.reg2(out)\n",
        "        # Get predictions using output layer\n",
        "        out = self.linear3(out)\n",
        "        return out\n",
        "    "
      ],
      "metadata": {
        "id": "FD9fNhk10TVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGCrhej7fjo6"
      },
      "source": [
        "You can now instantiate the model, and move it the appropriate device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQ67Qu19fjo6"
      },
      "outputs": [],
      "source": [
        "model = to_device(CIFAR10Model(in_size, hidden_size_1, hidden_size_2, out_size), device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6jFbJxdfjo7"
      },
      "source": [
        "Before you train the model, it's a good idea to check the validation loss & accuracy with the initial set of weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLCgckaWfjo7"
      },
      "outputs": [],
      "source": [
        "history = [evaluate(model, val_loader)]\n",
        "history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7wGytOefjo8"
      },
      "source": [
        "**Q: Train the model using the `fit` function to reduce the validation loss & improve accuracy.**\n",
        "\n",
        "Leverage the interactive nature of Jupyter to train the model in multiple phases, adjusting the no. of epochs & learning rate each time based on the result of the previous training phase."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history += fit(50, 0.01, model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "3oh9U7vXvTQH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fSQtTTDfjo9"
      },
      "outputs": [],
      "source": [
        "#history += fit(20, 0.1, model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cOZunDKfjo9"
      },
      "outputs": [],
      "source": [
        "#history += fit(20, 0.05, model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zh3GGfoSfjo-"
      },
      "outputs": [],
      "source": [
        "#history += fit(10, 0.1, model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIzLAlHyfjo-"
      },
      "outputs": [],
      "source": [
        "#history += fit(20, 0.01, model, train_loader, val_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#history += fit(10, 0.005, model, train_loader, val_loader)"
      ],
      "metadata": {
        "id": "a3LAA8GNC4uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2BDE-g5fjo_"
      },
      "source": [
        "Plot the losses and the accuracies to check if you're starting to hit the limits of how well your model can perform on this dataset. You can train some more if you can see the scope for further improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwA94kSSfjo_"
      },
      "outputs": [],
      "source": [
        "plot_losses(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4dn3D89fjpA"
      },
      "outputs": [],
      "source": [
        "plot_accuracies(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4wKvNV9fjpA"
      },
      "source": [
        "Finally, evaluate the model on the test dataset report its final performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7hOJ9eSfjpB"
      },
      "outputs": [],
      "source": [
        "evaluate(model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cahJSiB3fjpB"
      },
      "source": [
        "Are you happy with the accuracy? Record your results by completing the section below, then you can come back and try a different architecture & hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2bHGwn0fjpC"
      },
      "source": [
        "## Recoding your results\n",
        "\n",
        "As your perform multiple experiments, it's important to record the results in a systematic fashion, so that you can review them later and identify the best approaches that you might want to reproduce or build upon later. \n",
        "\n",
        "**Q: Describe the model's architecture with a short summary.**\n",
        "\n",
        "E.g. `\"3 layers (16,32,10)\"` (16, 32 and 10 represent output sizes of each layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEs_IcMWfjpC"
      },
      "outputs": [],
      "source": [
        "arch = \"3 layers (64,32,10)\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFaBVmiofjpC"
      },
      "source": [
        "**Q: Provide the list of learning rates used while training.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q86FXnRhfjpD"
      },
      "outputs": [],
      "source": [
        "lrs = [0.2,0.1, 0.05, 0.01]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PBCZMHDfjpD"
      },
      "source": [
        "**Q: Provide the list of no. of epochs used while training.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DcnVUIS8fjpD"
      },
      "outputs": [],
      "source": [
        "epochs = [10,5,5,10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6AgEosvfjpE"
      },
      "source": [
        "**Q: What were the final test accuracy & test loss?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kyl8aha7fjpE"
      },
      "outputs": [],
      "source": [
        "test_acc = 0.5064\n",
        "test_loss = 1.408"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FKqVuLffjpE"
      },
      "source": [
        "Finally, let's save the trained model weights to disk, so we can use this model later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M4sdaywYfjpE"
      },
      "outputs": [],
      "source": [
        "torch.save(model.state_dict(), 'cifar10-feedforwardv2.pth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S81mEUZIfjpI"
      },
      "source": [
        "Once committed, you can find the recorded metrics & hyperprameters in the \"Records\" tab on Jovian. You can find the saved model weights in the \"Files\" tab.\n",
        "\n",
        "## Continued experimentation\n",
        "\n",
        "Now go back up to the **\"Training the model\"** section, and try another network architecture with a different set of hyperparameters. As you try different experiments, you will start to build an undestanding of how the different architectures & hyperparameters affect the final result. Don't worry if you can't get to very high accuracy, we'll make some fundamental changes to our model in the next lecture.\n",
        "\n",
        "Once you have tried multiple experiments, you can compare your results using the **\"Compare\"** button on Jovian.\n",
        "\n",
        "![compare-example](https://i.imgur.com/ltdYnSN.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXKVTFjgfjpI"
      },
      "source": [
        "## (Optional) Write a blog post\n",
        "\n",
        "Writing a blog post is the best way to further improve your understanding of deep learning & model training, because it forces you to articulate your thoughts clearly. Here'are some ideas for a blog post:\n",
        "- Report the results given by different architectures on the CIFAR10 dataset\n",
        "- Apply this training pipeline to a different dataset (it doesn't have to be images, or a classification problem) \n",
        "- Improve upon your model from Assignment 2 using a feedfoward neural network, and write a sequel to your previous blog post\n",
        "- Share some Strategies for picking good hyperparameters for deep learning\n",
        "- Present a summary of the different steps involved in training a deep learning model with PyTorch\n",
        "- Implement the same model using a different deep learning library e.g. Keras ( https://keras.io/ ), and present a comparision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_gfbTMyfjpJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.6"
    },
    "colab": {
      "name": "CIFAR_dataset _deep learning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "S81mEUZIfjpI"
      ],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}